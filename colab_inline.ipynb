{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e99e454",
   "metadata": {},
   "source": "# Seq2Seq Code Generation — Google Colab (Inline Edition)\n### All source code lives directly in this notebook — edit any cell and re-run to update that file\n**Before running:** `Runtime → Change runtime type → GPU (T4)`"
  },
  {
   "cell_type": "markdown",
   "id": "e26256ef",
   "metadata": {},
   "source": "## Step 1 — Install Dependencies"
  },
  {
   "cell_type": "code",
   "id": "d39c6955",
   "metadata": {},
   "source": "!pip install datasets sacrebleu seaborn pyyaml tqdm -q\nprint(\"All dependencies installed!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf4d7edb",
   "metadata": {},
   "source": "## Step 2 — Check GPU\nIf you see \"No GPU\", go to `Runtime → Change runtime type → T4 GPU` and re-run."
  },
  {
   "cell_type": "code",
   "id": "f3cfa619",
   "metadata": {},
   "source": "import torch\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    print(\"Training will be fast!\")\nelse:\n    print(\"No GPU — go to Runtime → Change runtime type → T4 GPU\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03a35c23",
   "metadata": {},
   "source": "## Step 3 — Source Files\nEach cell below writes one file to disk.\n**Edit the code here in Colab, then re-run the cell to save your changes.**"
  },
  {
   "cell_type": "code",
   "id": "f643b423",
   "metadata": {},
   "source": "import os\nos.makedirs(\"models\", exist_ok=True)\nprint(\"models/ directory ready\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7161acda",
   "metadata": {},
   "source": "### models/__init__.py"
  },
  {
   "cell_type": "code",
   "id": "1275f442",
   "metadata": {},
   "source": "%%writefile models/__init__.py\n\"\"\"\nModels package initialization\n\"\"\"\nfrom .vanilla_rnn import create_vanilla_seq2seq\nfrom .lstm import create_lstm_seq2seq\nfrom .attention_lstm import create_attention_seq2seq\n\n__all__ = [\n    'create_vanilla_seq2seq',\n    'create_lstm_seq2seq',\n    'create_attention_seq2seq'\n]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ca560d1",
   "metadata": {},
   "source": "### models/vanilla_rnn.py"
  },
  {
   "cell_type": "code",
   "id": "646e6c59",
   "metadata": {},
   "source": "%%writefile models/vanilla_rnn.py\n\"\"\"\nVanilla RNN-based Seq2Seq Model\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport random\n\n\nclass EncoderRNN(nn.Module):\n    \"\"\"Vanilla RNN Encoder\"\"\"\n    \n    def __init__(self, input_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(EncoderRNN, self).__init__()\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_seq, input_lengths):\n        \"\"\"\n        Args:\n            input_seq: (batch_size, seq_len)\n            input_lengths: (batch_size,)\n        \n        Returns:\n            outputs: (batch_size, seq_len, hidden_dim)\n            hidden: (1, batch_size, hidden_dim)\n        \"\"\"\n        embedded = self.dropout(self.embedding(input_seq))\n        \n        # Pack padded sequence\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        \n        outputs, hidden = self.rnn(packed)\n        \n        # Unpack\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        \n        return outputs, hidden\n\n\nclass DecoderRNN(nn.Module):\n    \"\"\"Vanilla RNN Decoder\"\"\"\n    \n    def __init__(self, output_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(DecoderRNN, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.output_size = output_size\n        \n        self.embedding = nn.Embedding(output_size, embedding_dim, padding_idx=0)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.out = nn.Linear(hidden_dim, output_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_token, hidden):\n        \"\"\"\n        Args:\n            input_token: (batch_size, 1)\n            hidden: (1, batch_size, hidden_dim)\n        \n        Returns:\n            output: (batch_size, output_size)\n            hidden: (1, batch_size, hidden_dim)\n        \"\"\"\n        embedded = self.dropout(self.embedding(input_token))\n        \n        output, hidden = self.rnn(embedded, hidden)\n        \n        output = self.out(output.squeeze(1))\n        \n        return output, hidden\n\n\nclass VanillaSeq2Seq(nn.Module):\n    \"\"\"Vanilla RNN Seq2Seq Model\"\"\"\n    \n    def __init__(self, encoder, decoder, device):\n        super(VanillaSeq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n        \"\"\"\n        Args:\n            src: (batch_size, src_len)\n            src_lengths: (batch_size,)\n            tgt: (batch_size, tgt_len)\n            teacher_forcing_ratio: probability of using teacher forcing\n        \n        Returns:\n            outputs: (batch_size, tgt_len, output_size)\n        \"\"\"\n        batch_size = src.shape[0]\n        tgt_len = tgt.shape[1]\n        tgt_vocab_size = self.decoder.output_size\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n        \n        # Encode\n        _, hidden = self.encoder(src, src_lengths)\n        \n        # First input to decoder is SOS token\n        input_token = tgt[:, 0].unsqueeze(1)\n        \n        for t in range(1, tgt_len):\n            output, hidden = self.decoder(input_token, hidden)\n            outputs[:, t] = output\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n        \n        return outputs\n    \n    def generate(self, src, src_lengths, max_length, sos_token):\n        \"\"\"\n        Generate output sequence\n        \n        Args:\n            src: (batch_size, src_len)\n            src_lengths: (batch_size,)\n            max_length: maximum length of generated sequence\n            sos_token: start of sequence token\n        \n        Returns:\n            outputs: (batch_size, max_length)\n        \"\"\"\n        batch_size = src.shape[0]\n        \n        # Encode\n        _, hidden = self.encoder(src, src_lengths)\n        \n        # Start with SOS token\n        input_token = torch.tensor([[sos_token]] * batch_size).to(self.device)\n        \n        outputs = []\n        \n        for _ in range(max_length):\n            output, hidden = self.decoder(input_token, hidden)\n            top1 = output.argmax(1)\n            outputs.append(top1.unsqueeze(1))\n            input_token = top1.unsqueeze(1)\n        \n        return torch.cat(outputs, dim=1)\n\n\ndef create_vanilla_seq2seq(src_vocab_size, tgt_vocab_size, config, device):\n    \"\"\"Create Vanilla RNN Seq2Seq model\"\"\"\n    encoder = EncoderRNN(\n        src_vocab_size,\n        config['model']['embedding_dim'],\n        config['model']['hidden_dim'],\n        config['model']['dropout']\n    )\n    \n    decoder = DecoderRNN(\n        tgt_vocab_size,\n        config['model']['embedding_dim'],\n        config['model']['hidden_dim'],\n        config['model']['dropout']\n    )\n    \n    model = VanillaSeq2Seq(encoder, decoder, device).to(device)\n    \n    return model\n\n\nif __name__ == '__main__':\n    # Test model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    config = {\n        'model': {\n            'embedding_dim': 256,\n            'hidden_dim': 256,\n            'dropout': 0.3\n        }\n    }\n    \n    model = create_vanilla_seq2seq(5000, 5000, config, device)\n    \n    # Test forward pass\n    src = torch.randint(0, 5000, (32, 20)).to(device)\n    tgt = torch.randint(0, 5000, (32, 30)).to(device)\n    src_lengths = torch.tensor([20] * 32)\n    \n    outputs = model(src, src_lengths, tgt)\n    print(\"Output shape:\", outputs.shape)\n    \n    # Test generation\n    generated = model.generate(src, src_lengths, 30, 1)\n    print(\"Generated shape:\", generated.shape)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7412f115",
   "metadata": {},
   "source": "### models/lstm.py"
  },
  {
   "cell_type": "code",
   "id": "fd575897",
   "metadata": {},
   "source": "%%writefile models/lstm.py\n\"\"\"\nLSTM-based Seq2Seq Model\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport random\n\n\nclass EncoderLSTM(nn.Module):\n    \"\"\"LSTM Encoder\"\"\"\n    \n    def __init__(self, input_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(EncoderLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_seq, input_lengths):\n        \"\"\"\n        Args:\n            input_seq: (batch_size, seq_len)\n            input_lengths: (batch_size,)\n        \n        Returns:\n            outputs: (batch_size, seq_len, hidden_dim)\n            hidden: tuple of (h_n, c_n) each (1, batch_size, hidden_dim)\n        \"\"\"\n        embedded = self.dropout(self.embedding(input_seq))\n        \n        # Pack padded sequence\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        \n        outputs, (hidden, cell) = self.lstm(packed)\n        \n        # Unpack\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        \n        return outputs, (hidden, cell)\n\n\nclass DecoderLSTM(nn.Module):\n    \"\"\"LSTM Decoder\"\"\"\n    \n    def __init__(self, output_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(DecoderLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.output_size = output_size\n        \n        self.embedding = nn.Embedding(output_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.out = nn.Linear(hidden_dim, output_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_token, hidden, cell):\n        \"\"\"\n        Args:\n            input_token: (batch_size, 1)\n            hidden: (1, batch_size, hidden_dim)\n            cell: (1, batch_size, hidden_dim)\n        \n        Returns:\n            output: (batch_size, output_size)\n            hidden: (1, batch_size, hidden_dim)\n            cell: (1, batch_size, hidden_dim)\n        \"\"\"\n        embedded = self.dropout(self.embedding(input_token))\n        \n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        \n        output = self.out(output.squeeze(1))\n        \n        return output, hidden, cell\n\n\nclass LSTMSeq2Seq(nn.Module):\n    \"\"\"LSTM Seq2Seq Model\"\"\"\n    \n    def __init__(self, encoder, decoder, device):\n        super(LSTMSeq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n        \"\"\"\n        Args:\n            src: (batch_size, src_len)\n            src_lengths: (batch_size,)\n            tgt: (batch_size, tgt_len)\n            teacher_forcing_ratio: probability of using teacher forcing\n        \n        Returns:\n            outputs: (batch_size, tgt_len, output_size)\n        \"\"\"\n        batch_size = src.shape[0]\n        tgt_len = tgt.shape[1]\n        tgt_vocab_size = self.decoder.output_size\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n        \n        # Encode\n        _, (hidden, cell) = self.encoder(src, src_lengths)\n        \n        # First input to decoder is SOS token\n        input_token = tgt[:, 0].unsqueeze(1)\n        \n        for t in range(1, tgt_len):\n            output, hidden, cell = self.decoder(input_token, hidden, cell)\n            outputs[:, t] = output\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n        \n        return outputs\n    \n    def generate(self, src, src_lengths, max_length, sos_token):\n        \"\"\"\n        Generate output sequence\n        \n        Args:\n            src: (batch_size, src_len)\n            src_lengths: (batch_size,)\n            max_length: maximum length of generated sequence\n            sos_token: start of sequence token\n        \n        Returns:\n            outputs: (batch_size, max_length)\n        \"\"\"\n        batch_size = src.shape[0]\n        \n        # Encode\n        _, (hidden, cell) = self.encoder(src, src_lengths)\n        \n        # Start with SOS token\n        input_token = torch.tensor([[sos_token]] * batch_size).to(self.device)\n        \n        outputs = []\n        \n        for _ in range(max_length):\n            output, hidden, cell = self.decoder(input_token, hidden, cell)\n            top1 = output.argmax(1)\n            outputs.append(top1.unsqueeze(1))\n            input_token = top1.unsqueeze(1)\n        \n        return torch.cat(outputs, dim=1)\n\n\ndef create_lstm_seq2seq(src_vocab_size, tgt_vocab_size, config, device):\n    \"\"\"Create LSTM Seq2Seq model\"\"\"\n    encoder = EncoderLSTM(\n        src_vocab_size,\n        config['model']['embedding_dim'],\n        config['model']['hidden_dim'],\n        config['model']['dropout']\n    )\n    \n    decoder = DecoderLSTM(\n        tgt_vocab_size,\n        config['model']['embedding_dim'],\n        config['model']['hidden_dim'],\n        config['model']['dropout']\n    )\n    \n    model = LSTMSeq2Seq(encoder, decoder, device).to(device)\n    \n    return model\n\n\nif __name__ == '__main__':\n    # Test model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    config = {\n        'model': {\n            'embedding_dim': 256,\n            'hidden_dim': 256,\n            'dropout': 0.3\n        }\n    }\n    \n    model = create_lstm_seq2seq(5000, 5000, config, device)\n    \n    # Test forward pass\n    src = torch.randint(0, 5000, (32, 20)).to(device)\n    tgt = torch.randint(0, 5000, (32, 30)).to(device)\n    src_lengths = torch.tensor([20] * 32)\n    \n    outputs = model(src, src_lengths, tgt)\n    print(\"Output shape:\", outputs.shape)\n    \n    # Test generation\n    generated = model.generate(src, src_lengths, 30, 1)\n    print(\"Generated shape:\", generated.shape)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c148e445",
   "metadata": {},
   "source": "### models/attention_lstm.py"
  },
  {
   "cell_type": "code",
   "id": "351e1395",
   "metadata": {},
   "source": "%%writefile models/attention_lstm.py\n\"\"\"\nLSTM with Bahdanau Attention Seq2Seq Model\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\n\n\nclass EncoderBiLSTM(nn.Module):\n    \"\"\"Bidirectional LSTM Encoder\"\"\"\n    \n    def __init__(self, input_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(EncoderBiLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.dropout = nn.Dropout(dropout)\n        \n        # Linear layer to project bidirectional hidden state to decoder hidden size\n        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.fc_cell = nn.Linear(hidden_dim * 2, hidden_dim)\n    \n    def forward(self, input_seq, input_lengths):\n        \"\"\"\n        Args:\n            input_seq: (batch_size, seq_len)\n            input_lengths: (batch_size,)\n        \n        Returns:\n            outputs: (batch_size, seq_len, hidden_dim * 2)\n            hidden: (1, batch_size, hidden_dim)\n            cell: (1, batch_size, hidden_dim)\n        \"\"\"\n        embedded = self.dropout(self.embedding(input_seq))\n        \n        # Pack padded sequence\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        \n        outputs, (hidden, cell) = self.lstm(packed)\n        \n        # Unpack\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        \n        # hidden and cell are (2, batch_size, hidden_dim) for bidirectional\n        # Concatenate forward and backward and project to decoder size\n        hidden = torch.tanh(self.fc_hidden(torch.cat((hidden[0], hidden[1]), dim=1))).unsqueeze(0)\n        cell = torch.tanh(self.fc_cell(torch.cat((cell[0], cell[1]), dim=1))).unsqueeze(0)\n        \n        return outputs, hidden, cell\n\n\nclass BahdanauAttention(nn.Module):\n    \"\"\"Bahdanau (Additive) Attention Mechanism\"\"\"\n    \n    def __init__(self, hidden_dim, encoder_dim):\n        super(BahdanauAttention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.encoder_dim = encoder_dim\n        \n        # Attention layers\n        self.attn_hidden = nn.Linear(hidden_dim, hidden_dim)\n        self.attn_encoder = nn.Linear(encoder_dim, hidden_dim)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n    \n    def forward(self, hidden, encoder_outputs, mask=None):\n        \"\"\"\n        Args:\n            hidden: (batch_size, hidden_dim) - decoder hidden state\n            encoder_outputs: (batch_size, src_len, encoder_dim) - all encoder outputs\n            mask: (batch_size, src_len) - mask for padding\n        \n        Returns:\n            context: (batch_size, encoder_dim) - weighted context vector\n            attention_weights: (batch_size, src_len) - attention weights\n        \"\"\"\n        src_len = encoder_outputs.shape[1]\n        \n        # Repeat hidden state src_len times\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # (batch_size, src_len, hidden_dim)\n        \n        # Calculate attention energy\n        energy = torch.tanh(\n            self.attn_hidden(hidden) + self.attn_encoder(encoder_outputs)\n        )  # (batch_size, src_len, hidden_dim)\n        \n        # Calculate attention scores\n        attention = self.v(energy).squeeze(2)  # (batch_size, src_len)\n        \n        # Apply mask if provided\n        if mask is not None:\n            attention = attention.masked_fill(mask == 0, -1e10)\n        \n        # Softmax to get attention weights\n        attention_weights = F.softmax(attention, dim=1)  # (batch_size, src_len)\n        \n        # Calculate context vector\n        context = torch.bmm(\n            attention_weights.unsqueeze(1),\n            encoder_outputs\n        ).squeeze(1)  # (batch_size, encoder_dim)\n        \n        return context, attention_weights\n\n\nclass AttentionDecoderLSTM(nn.Module):\n    \"\"\"LSTM Decoder with Attention\"\"\"\n    \n    def __init__(self, output_size, embedding_dim, hidden_dim, encoder_dim, dropout=0.3):\n        super(AttentionDecoderLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.output_size = output_size\n        self.encoder_dim = encoder_dim\n        \n        self.embedding = nn.Embedding(output_size, embedding_dim, padding_idx=0)\n        self.attention = BahdanauAttention(hidden_dim, encoder_dim)\n        \n        # LSTM input is embedding + context vector\n        self.lstm = nn.LSTM(embedding_dim + encoder_dim, hidden_dim, batch_first=True)\n        \n        # Output layer\n        self.out = nn.Linear(hidden_dim + encoder_dim + embedding_dim, output_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_token, hidden, cell, encoder_outputs, mask=None):\n        \"\"\"\n        Args:\n            input_token: (batch_size, 1)\n            hidden: (1, batch_size, hidden_dim)\n            cell: (1, batch_size, hidden_dim)\n            encoder_outputs: (batch_size, src_len, encoder_dim)\n            mask: (batch_size, src_len)\n        \n        Returns:\n            output: (batch_size, output_size)\n            hidden: (1, batch_size, hidden_dim)\n            cell: (1, batch_size, hidden_dim)\n            attention_weights: (batch_size, src_len)\n        \"\"\"\n        embedded = self.dropout(self.embedding(input_token))  # (batch_size, 1, embedding_dim)\n        \n        # Calculate attention\n        context, attention_weights = self.attention(\n            hidden.squeeze(0),\n            encoder_outputs,\n            mask\n        )  # context: (batch_size, encoder_dim)\n        \n        # Concatenate embedding and context\n        lstm_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)  # (batch_size, 1, embedding_dim + encoder_dim)\n        \n        # LSTM forward\n        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n        \n        # Concatenate output, context, and embedding for final prediction\n        output = output.squeeze(1)  # (batch_size, hidden_dim)\n        embedded = embedded.squeeze(1)  # (batch_size, embedding_dim)\n        \n        pred_input = torch.cat((output, context, embedded), dim=1)  # (batch_size, hidden_dim + encoder_dim + embedding_dim)\n        prediction = self.out(pred_input)  # (batch_size, output_size)\n        \n        return prediction, hidden, cell, attention_weights\n\n\nclass AttentionSeq2Seq(nn.Module):\n    \"\"\"LSTM with Attention Seq2Seq Model\"\"\"\n    \n    def __init__(self, encoder, decoder, device):\n        super(AttentionSeq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def create_mask(self, src, src_lengths):\n        \"\"\"Create mask for padding\"\"\"\n        mask = torch.zeros_like(src, dtype=torch.bool)\n        for i, length in enumerate(src_lengths):\n            mask[i, :length] = 1\n        return mask\n    \n    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n        \"\"\"\n        Args:\n            src: (batch_size, src_len)\n            src_lengths: (batch_size,)\n            tgt: (batch_size, tgt_len)\n            teacher_forcing_ratio: probability of using teacher forcing\n        \n        Returns:\n            outputs: (batch_size, tgt_len, output_size)\n            attentions: (batch_size, tgt_len, src_len)\n        \"\"\"\n        batch_size = src.shape[0]\n        tgt_len = tgt.shape[1]\n        tgt_vocab_size = self.decoder.output_size\n        src_len = src.shape[1]\n        \n        # Tensors to store decoder outputs and attention weights\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n        attentions = torch.zeros(batch_size, tgt_len, src_len).to(self.device)\n        \n        # Create mask\n        mask = self.create_mask(src, src_lengths).to(self.device)\n        \n        # Encode\n        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n        \n        # First input to decoder is SOS token\n        input_token = tgt[:, 0].unsqueeze(1)\n        \n        for t in range(1, tgt_len):\n            output, hidden, cell, attention_weights = self.decoder(\n                input_token, hidden, cell, encoder_outputs, mask\n            )\n            outputs[:, t] = output\n            attentions[:, t] = attention_weights\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n        \n        return outputs, attentions\n    \n    def generate(self, src, src_lengths, max_length, sos_token):\n        \"\"\"\n        Generate output sequence\n        \n        Args:\n            src: (batch_size, src_len)\n            src_lengths: (batch_size,)\n            max_length: maximum length of generated sequence\n            sos_token: start of sequence token\n        \n        Returns:\n            outputs: (batch_size, max_length)\n            attentions: (batch_size, max_length, src_len)\n        \"\"\"\n        batch_size = src.shape[0]\n        src_len = src.shape[1]\n        \n        # Create mask\n        mask = self.create_mask(src, src_lengths).to(self.device)\n        \n        # Encode\n        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n        \n        # Start with SOS token\n        input_token = torch.tensor([[sos_token]] * batch_size).to(self.device)\n        \n        outputs = []\n        attentions = torch.zeros(batch_size, max_length, src_len).to(self.device)\n        \n        for t in range(max_length):\n            output, hidden, cell, attention_weights = self.decoder(\n                input_token, hidden, cell, encoder_outputs, mask\n            )\n            attentions[:, t] = attention_weights\n            top1 = output.argmax(1)\n            outputs.append(top1.unsqueeze(1))\n            input_token = top1.unsqueeze(1)\n        \n        return torch.cat(outputs, dim=1), attentions\n\n\ndef create_attention_seq2seq(src_vocab_size, tgt_vocab_size, config, device):\n    \"\"\"Create LSTM with Attention Seq2Seq model\"\"\"\n    encoder = EncoderBiLSTM(\n        src_vocab_size,\n        config['model']['embedding_dim'],\n        config['model']['hidden_dim'],\n        config['model']['dropout']\n    )\n    \n    decoder = AttentionDecoderLSTM(\n        tgt_vocab_size,\n        config['model']['embedding_dim'],\n        config['model']['hidden_dim'],\n        config['model']['hidden_dim'] * 2,  # bidirectional encoder\n        config['model']['dropout']\n    )\n    \n    model = AttentionSeq2Seq(encoder, decoder, device).to(device)\n    \n    return model\n\n\nif __name__ == '__main__':\n    # Test model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    config = {\n        'model': {\n            'embedding_dim': 256,\n            'hidden_dim': 256,\n            'dropout': 0.3\n        }\n    }\n    \n    model = create_attention_seq2seq(5000, 5000, config, device)\n    \n    # Test forward pass\n    src = torch.randint(0, 5000, (32, 20)).to(device)\n    tgt = torch.randint(0, 5000, (32, 30)).to(device)\n    src_lengths = torch.tensor([20] * 32)\n    \n    outputs, attentions = model(src, src_lengths, tgt)\n    print(\"Output shape:\", outputs.shape)\n    print(\"Attention shape:\", attentions.shape)\n    \n    # Test generation\n    generated, gen_attentions = model.generate(src, src_lengths, 30, 1)\n    print(\"Generated shape:\", generated.shape)\n    print(\"Generated attention shape:\", gen_attentions.shape)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a53e3662",
   "metadata": {},
   "source": "### data_loader.py"
  },
  {
   "cell_type": "code",
   "id": "5f61a5f7",
   "metadata": {},
   "source": "%%writefile data_loader.py\n\"\"\"\nData loading and preprocessing for CodeSearchNet dataset\n\"\"\"\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom collections import Counter\nimport re\nfrom typing import List, Tuple, Dict\nimport pickle\nimport os\n\n\nclass Vocabulary:\n    \"\"\"Builds and manages vocabulary for source and target sequences\"\"\"\n    \n    def __init__(self, max_vocab_size=10000):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.word_counts = Counter()\n        self.max_vocab_size = max_vocab_size\n        \n        # Special tokens\n        self.PAD_token = 0\n        self.SOS_token = 1\n        self.EOS_token = 2\n        self.UNK_token = 3\n        \n        self.word2idx['<PAD>'] = self.PAD_token\n        self.word2idx['<SOS>'] = self.SOS_token\n        self.word2idx['<EOS>'] = self.EOS_token\n        self.word2idx['<UNK>'] = self.UNK_token\n        \n        self.idx2word[self.PAD_token] = '<PAD>'\n        self.idx2word[self.SOS_token] = '<SOS>'\n        self.idx2word[self.EOS_token] = '<EOS>'\n        self.idx2word[self.UNK_token] = '<UNK>'\n        \n        self.n_words = 4\n    \n    def add_sentence(self, sentence: str):\n        \"\"\"Add all words in a sentence to vocabulary\"\"\"\n        for word in self.tokenize(sentence):\n            self.word_counts[word] += 1\n    \n    def build_vocab(self):\n        \"\"\"Build vocabulary from word counts, keeping most common words\"\"\"\n        # Get most common words\n        most_common = self.word_counts.most_common(self.max_vocab_size - 4)\n        \n        for word, _ in most_common:\n            if word not in self.word2idx:\n                self.word2idx[word] = self.n_words\n                self.idx2word[self.n_words] = word\n                self.n_words += 1\n    \n    @staticmethod\n    def tokenize(text: str) -> List[str]:\n        \"\"\"Simple whitespace tokenization with some preprocessing\"\"\"\n        # Basic preprocessing\n        text = text.lower().strip()\n        # Split on whitespace and basic punctuation\n        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n        return tokens\n    \n    def encode(self, sentence: str) -> List[int]:\n        \"\"\"Convert sentence to list of indices\"\"\"\n        tokens = self.tokenize(sentence)\n        return [self.word2idx.get(token, self.UNK_token) for token in tokens]\n    \n    def decode(self, indices: List[int]) -> str:\n        \"\"\"Convert list of indices back to sentence\"\"\"\n        words = []\n        for idx in indices:\n            if idx == self.EOS_token:\n                break\n            if idx not in [self.PAD_token, self.SOS_token]:\n                words.append(self.idx2word.get(idx, '<UNK>'))\n        return ' '.join(words)\n    \n    def save(self, path: str):\n        \"\"\"Save vocabulary to file\"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump({\n                'word2idx': self.word2idx,\n                'idx2word': self.idx2word,\n                'word_counts': self.word_counts,\n                'max_vocab_size': self.max_vocab_size,\n                'n_words': self.n_words\n            }, f)\n    \n    def load(self, path: str):\n        \"\"\"Load vocabulary from file\"\"\"\n        with open(path, 'rb') as f:\n            data = pickle.load(f)\n            self.word2idx = data['word2idx']\n            self.idx2word = data['idx2word']\n            self.word_counts = data['word_counts']\n            self.max_vocab_size = data['max_vocab_size']\n            self.n_words = data['n_words']\n\n\nclass CodeSearchNetDataset(Dataset):\n    \"\"\"PyTorch Dataset for CodeSearchNet data\"\"\"\n    \n    def __init__(self, data, src_vocab, tgt_vocab, max_src_len, max_tgt_len):\n        self.data = data\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.max_src_len = max_src_len\n        self.max_tgt_len = max_tgt_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Encode source (docstring)\n        src_indices = self.src_vocab.encode(item['docstring'])\n        # Truncate if too long\n        src_indices = src_indices[:self.max_src_len]\n        # Add EOS token\n        src_indices.append(self.src_vocab.EOS_token)\n        \n        # Encode target (code)\n        tgt_indices = self.tgt_vocab.encode(item['code'])\n        # Truncate if too long\n        tgt_indices = tgt_indices[:self.max_tgt_len]\n        # Add SOS and EOS tokens\n        tgt_indices = [self.tgt_vocab.SOS_token] + tgt_indices + [self.tgt_vocab.EOS_token]\n        \n        return {\n            'src': torch.tensor(src_indices, dtype=torch.long),\n            'tgt': torch.tensor(tgt_indices, dtype=torch.long),\n            'src_text': item['docstring'],\n            'tgt_text': item['code']\n        }\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to pad sequences in a batch\"\"\"\n    src_batch = [item['src'] for item in batch]\n    tgt_batch = [item['tgt'] for item in batch]\n    src_texts = [item['src_text'] for item in batch]\n    tgt_texts = [item['tgt_text'] for item in batch]\n    \n    # Pad sequences\n    src_lengths = torch.tensor([len(s) for s in src_batch])\n    tgt_lengths = torch.tensor([len(t) for t in tgt_batch])\n    \n    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n    \n    return {\n        'src': src_padded,\n        'tgt': tgt_padded,\n        'src_lengths': src_lengths,\n        'tgt_lengths': tgt_lengths,\n        'src_texts': src_texts,\n        'tgt_texts': tgt_texts\n    }\n\n\ndef load_and_prepare_data(config):\n    \"\"\"\n    Load CodeSearchNet dataset and prepare vocabularies\n    \n    Args:\n        config: Configuration dictionary\n    \n    Returns:\n        tuple: (train_loader, val_loader, test_loader, src_vocab, tgt_vocab)\n    \"\"\"\n    print(\"Loading CodeSearchNet dataset...\")\n    \n    # Load dataset from Hugging Face\n    dataset = load_dataset(\n        config['dataset']['name'],\n        split='train',\n        cache_dir=config['dataset']['cache_dir']\n    )\n    \n    print(f\"Total dataset size: {len(dataset)}\")\n    \n    # Take larger subset initially (we'll filter and then select what we need)\n    total_needed = config['dataset']['train_size'] + config['dataset']['val_size'] + config['dataset']['test_size']\n    # Get 5x more than needed to account for filtering\n    initial_sample = min(total_needed * 5, len(dataset))\n    dataset = dataset.shuffle(seed=42).select(range(initial_sample))\n    \n    # Filter out examples that are too long or empty\n    def filter_fn(example):\n        try:\n            # Handle different possible field names\n            doc = example.get('func_documentation_string') or example.get('docstring') or example.get('doc')\n            code = example.get('func_code_string') or example.get('code') or example.get('function')\n            \n            if not doc or not code:\n                return False\n            \n            # Check if strings are not empty after stripping\n            doc = str(doc).strip()\n            code = str(code).strip()\n            \n            if not doc or not code:\n                return False\n            \n            doc_len = len(Vocabulary.tokenize(doc))\n            code_len = len(Vocabulary.tokenize(code))\n            \n            return (doc_len > 2 and doc_len <= config['dataset']['max_docstring_length'] and\n                    code_len > 2 and code_len <= config['dataset']['max_code_length'])\n        except Exception as e:\n            return False\n    \n    print(\"Filtering dataset...\")\n    dataset = dataset.filter(filter_fn)\n    print(f\"After filtering: {len(dataset)} examples\")\n    \n    # Check if we have enough data\n    if len(dataset) == 0:\n        raise ValueError(\"No examples passed filtering! The dataset might have different field names or all examples were too long.\")\n    \n    # Convert to simpler format\n    processed_data = []\n    for item in dataset:\n        # Handle different possible field names\n        doc = item.get('func_documentation_string') or item.get('docstring') or item.get('doc')\n        code = item.get('func_code_string') or item.get('code') or item.get('function')\n        \n        processed_data.append({\n            'docstring': str(doc).strip(),\n            'code': str(code).strip()\n        })\n        \n        # Stop if we have enough examples\n        if len(processed_data) >= total_needed:\n            break\n    \n    # Split into train/val/test\n    train_size = config['dataset']['train_size']\n    val_size = config['dataset']['val_size']\n    \n    train_data = processed_data[:train_size]\n    val_data = processed_data[train_size:train_size + val_size]\n    test_data = processed_data[train_size + val_size:train_size + val_size + config['dataset']['test_size']]\n    \n    print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n    \n    # Check if we have enough data\n    if len(train_data) == 0:\n        raise ValueError(f\"No training data! Got {len(processed_data)} examples after filtering, but needed at least {train_size}. Try reducing the dataset sizes in config.yaml or increasing max_docstring_length/max_code_length.\")\n    \n    if len(train_data) < train_size:\n        print(f\"⚠️  Warning: Only got {len(train_data)} training examples (requested {train_size})\")\n        print(f\"   Continuing with available data...\")\n    \n    # Build vocabularies\n    print(\"Building vocabularies...\")\n    src_vocab = Vocabulary(max_vocab_size=config['model']['max_vocab_size'])\n    tgt_vocab = Vocabulary(max_vocab_size=config['model']['max_vocab_size'])\n    \n    # Add all sentences to vocabulary\n    for item in train_data:\n        src_vocab.add_sentence(item['docstring'])\n        tgt_vocab.add_sentence(item['code'])\n    \n    src_vocab.build_vocab()\n    tgt_vocab.build_vocab()\n    \n    print(f\"Source vocabulary size: {src_vocab.n_words}\")\n    print(f\"Target vocabulary size: {tgt_vocab.n_words}\")\n    \n    # Create datasets\n    train_dataset = CodeSearchNetDataset(\n        train_data, src_vocab, tgt_vocab,\n        config['dataset']['max_docstring_length'],\n        config['dataset']['max_code_length']\n    )\n    val_dataset = CodeSearchNetDataset(\n        val_data, src_vocab, tgt_vocab,\n        config['dataset']['max_docstring_length'],\n        config['dataset']['max_code_length']\n    )\n    test_dataset = CodeSearchNetDataset(\n        test_data, src_vocab, tgt_vocab,\n        config['dataset']['max_docstring_length'],\n        config['dataset']['max_code_length']\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=0\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=0\n    )\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=0\n    )\n    \n    # Save vocabularies\n    os.makedirs(config['paths']['checkpoints'], exist_ok=True)\n    src_vocab.save(os.path.join(config['paths']['checkpoints'], 'src_vocab.pkl'))\n    tgt_vocab.save(os.path.join(config['paths']['checkpoints'], 'tgt_vocab.pkl'))\n    \n    return train_loader, val_loader, test_loader, src_vocab, tgt_vocab\n\n\nif __name__ == '__main__':\n    import yaml\n    \n    # Test data loading\n    with open('config.yaml', 'r') as f:\n        config = yaml.safe_load(f)\n    \n    train_loader, val_loader, test_loader, src_vocab, tgt_vocab = load_and_prepare_data(config)\n    \n    # Print sample batch\n    for batch in train_loader:\n        print(\"Source shape:\", batch['src'].shape)\n        print(\"Target shape:\", batch['tgt'].shape)\n        print(\"Source text:\", batch['src_texts'][0])\n        print(\"Target text:\", batch['tgt_texts'][0])\n        break\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "102245ae",
   "metadata": {},
   "source": "### train.py"
  },
  {
   "cell_type": "code",
   "id": "0d451540",
   "metadata": {},
   "source": "%%writefile train.py\n\"\"\"\nTraining script for Seq2Seq models\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport yaml\nimport os\nimport argparse\nimport json\nimport numpy as np\n\nfrom data_loader import load_and_prepare_data, Vocabulary\nfrom models import create_vanilla_seq2seq, create_lstm_seq2seq, create_attention_seq2seq\n\n\nclass Trainer:\n    \"\"\"Trainer class for Seq2Seq models\"\"\"\n    \n    def __init__(self, model, config, device, model_name, src_vocab, tgt_vocab):\n        self.model = model\n        self.config = config\n        self.device = device\n        self.model_name = model_name\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        \n        # Loss and optimizer\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n        self.optimizer = optim.Adam(\n            model.parameters(),\n            lr=config['training']['learning_rate']\n        )\n        \n        # Training history\n        self.train_losses = []\n        self.val_losses = []\n        \n        # Create directories\n        self.checkpoint_dir = os.path.join(config['paths']['checkpoints'], model_name)\n        self.log_dir = os.path.join(config['paths']['logs'], model_name)\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        os.makedirs(self.log_dir, exist_ok=True)\n    \n    def train_epoch(self, train_loader, epoch):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        epoch_loss = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n        \n        for batch_idx, batch in enumerate(progress_bar):\n            src = batch['src'].to(self.device)\n            tgt = batch['tgt'].to(self.device)\n            src_lengths = batch['src_lengths']\n            \n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            if 'attention' in self.model_name:\n                outputs, _ = self.model(\n                    src, src_lengths, tgt,\n                    teacher_forcing_ratio=self.config['training']['teacher_forcing_ratio']\n                )\n            else:\n                outputs = self.model(\n                    src, src_lengths, tgt,\n                    teacher_forcing_ratio=self.config['training']['teacher_forcing_ratio']\n                )\n            \n            # Calculate loss\n            # Reshape: outputs (batch_size, tgt_len, vocab_size) -> (batch_size * tgt_len, vocab_size)\n            # tgt (batch_size, tgt_len) -> (batch_size * tgt_len)\n            output_dim = outputs.shape[-1]\n            outputs = outputs[:, 1:].contiguous().view(-1, output_dim)\n            tgt = tgt[:, 1:].contiguous().view(-1)\n            \n            loss = self.criterion(outputs, tgt)\n            \n            # Backward pass\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(\n                self.model.parameters(),\n                self.config['training']['gradient_clip']\n            )\n            \n            self.optimizer.step()\n            \n            epoch_loss += loss.item()\n            \n            progress_bar.set_postfix({'loss': loss.item()})\n        \n        return epoch_loss / len(train_loader)\n    \n    def evaluate(self, val_loader):\n        \"\"\"Evaluate on validation set\"\"\"\n        self.model.eval()\n        epoch_loss = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validation'):\n                src = batch['src'].to(self.device)\n                tgt = batch['tgt'].to(self.device)\n                src_lengths = batch['src_lengths']\n                \n                # Forward pass\n                if 'attention' in self.model_name:\n                    outputs, _ = self.model(src, src_lengths, tgt, teacher_forcing_ratio=0)\n                else:\n                    outputs = self.model(src, src_lengths, tgt, teacher_forcing_ratio=0)\n                \n                # Calculate loss\n                output_dim = outputs.shape[-1]\n                outputs = outputs[:, 1:].contiguous().view(-1, output_dim)\n                tgt = tgt[:, 1:].contiguous().view(-1)\n                \n                loss = self.criterion(outputs, tgt)\n                epoch_loss += loss.item()\n        \n        return epoch_loss / len(val_loader)\n    \n    def train(self, train_loader, val_loader, num_epochs):\n        \"\"\"Full training loop\"\"\"\n        print(f\"\\nTraining {self.model_name}...\")\n        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n        \n        best_val_loss = float('inf')\n        \n        for epoch in range(num_epochs):\n            train_loss = self.train_epoch(train_loader, epoch)\n            val_loss = self.evaluate(val_loader)\n            \n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            \n            print(f'Epoch {epoch+1}/{num_epochs}:')\n            print(f'  Train Loss: {train_loss:.4f}')\n            print(f'  Val Loss:   {val_loss:.4f}')\n            \n            # Save checkpoint\n            if (epoch + 1) % self.config['training']['save_every'] == 0:\n                self.save_checkpoint(epoch, val_loss)\n            \n            # Save best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                self.save_checkpoint(epoch, val_loss, is_best=True)\n                print(f'  New best model saved!')\n        \n        # Save training history\n        self.save_training_history()\n        \n        print(f'\\nTraining completed for {self.model_name}!')\n        print(f'Best validation loss: {best_val_loss:.4f}')\n    \n    def save_checkpoint(self, epoch, val_loss, is_best=False):\n        \"\"\"Save model checkpoint\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'val_loss': val_loss,\n            'train_losses': self.train_losses,\n            'val_losses': self.val_losses,\n            'config': self.config,\n            'src_vocab': self.src_vocab,\n            'tgt_vocab': self.tgt_vocab\n        }\n        \n        if is_best:\n            path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n        else:\n            path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n        \n        torch.save(checkpoint, path)\n    \n    def save_training_history(self):\n        \"\"\"Save training history\"\"\"\n        history = {\n            'train_losses': self.train_losses,\n            'val_losses': self.val_losses\n        }\n        \n        path = os.path.join(self.log_dir, 'training_history.json')\n        with open(path, 'w') as f:\n            json.dump(history, f, indent=2)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Train Seq2Seq models')\n    parser.add_argument('--config', type=str, default='config.yaml',\n                        help='Path to config file')\n    parser.add_argument('--model', type=str, default='all',\n                        choices=['vanilla', 'lstm', 'attention', 'all'],\n                        help='Which model to train')\n    args = parser.parse_args()\n    \n    # Load config\n    with open(args.config, 'r') as f:\n        config = yaml.safe_load(f)\n    \n    # Set device\n    device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    \n    # Load data\n    print('Loading data...')\n    train_loader, val_loader, test_loader, src_vocab, tgt_vocab = load_and_prepare_data(config)\n    \n    src_vocab_size = src_vocab.n_words\n    tgt_vocab_size = tgt_vocab.n_words\n    \n    print(f'Source vocabulary size: {src_vocab_size}')\n    print(f'Target vocabulary size: {tgt_vocab_size}')\n    \n    # Determine which models to train\n    models_to_train = []\n    if args.model == 'all':\n        models_to_train = ['vanilla', 'lstm', 'attention']\n    else:\n        models_to_train = [args.model]\n    \n    # Train models\n    for model_name in models_to_train:\n        print(f'\\n{\"=\"*60}')\n        print(f'Training {model_name.upper()} model')\n        print(f'{\"=\"*60}')\n        \n        # Create model\n        if model_name == 'vanilla':\n            model = create_vanilla_seq2seq(src_vocab_size, tgt_vocab_size, config, device)\n        elif model_name == 'lstm':\n            model = create_lstm_seq2seq(src_vocab_size, tgt_vocab_size, config, device)\n        elif model_name == 'attention':\n            model = create_attention_seq2seq(src_vocab_size, tgt_vocab_size, config, device)\n        \n        # Create trainer and train\n        trainer = Trainer(model, config, device, model_name, src_vocab, tgt_vocab)\n        trainer.train(train_loader, val_loader, config['training']['num_epochs'])\n    \n    print('\\n' + '='*60)\n    print('All training completed!')\n    print('='*60)\n\n\nif __name__ == '__main__':\n    main()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04d17fad",
   "metadata": {},
   "source": "### evaluate.py"
  },
  {
   "cell_type": "code",
   "id": "01eeb87a",
   "metadata": {},
   "source": "%%writefile evaluate.py\n\"\"\"\nEvaluation metrics and testing script\n\"\"\"\nimport torch\nimport yaml\nimport os\nimport argparse\nimport json\nimport numpy as np\nfrom tqdm import tqdm\nfrom sacrebleu.metrics import BLEU\nfrom collections import defaultdict\n\nfrom data_loader import load_and_prepare_data, Vocabulary\nfrom models import create_vanilla_seq2seq, create_lstm_seq2seq, create_attention_seq2seq\n\n\nclass Evaluator:\n    \"\"\"Evaluation class for Seq2Seq models\"\"\"\n    \n    def __init__(self, model, config, device, model_name, src_vocab, tgt_vocab):\n        self.model = model\n        self.config = config\n        self.device = device\n        self.model_name = model_name\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        \n        # BLEU metric\n        self.bleu = BLEU()\n    \n    def load_checkpoint(self, checkpoint_path):\n        \"\"\"Load model checkpoint\"\"\"\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}')\n        print(f'Validation loss: {checkpoint[\"val_loss\"]:.4f}')\n    \n    def generate_sequences(self, data_loader):\n        \"\"\"Generate sequences for all examples in data loader\"\"\"\n        self.model.eval()\n        \n        all_predictions = []\n        all_references = []\n        all_src_texts = []\n        \n        with torch.no_grad():\n            for batch in tqdm(data_loader, desc='Generating'):\n                src = batch['src'].to(self.device)\n                src_lengths = batch['src_lengths']\n                src_texts = batch['src_texts']\n                tgt_texts = batch['tgt_texts']\n                \n                # Generate\n                max_length = self.config['dataset']['max_code_length'] + 2\n                if 'attention' in self.model_name:\n                    generated, _ = self.model.generate(\n                        src, src_lengths, max_length, self.tgt_vocab.SOS_token\n                    )\n                else:\n                    generated = self.model.generate(\n                        src, src_lengths, max_length, self.tgt_vocab.SOS_token\n                    )\n                \n                # Decode generated sequences\n                for i in range(generated.shape[0]):\n                    pred_indices = generated[i].cpu().tolist()\n                    pred_text = self.tgt_vocab.decode(pred_indices)\n                    \n                    all_predictions.append(pred_text)\n                    all_references.append(tgt_texts[i])\n                    all_src_texts.append(src_texts[i])\n        \n        return all_predictions, all_references, all_src_texts\n    \n    def calculate_token_accuracy(self, predictions, references):\n        \"\"\"Calculate token-level accuracy\"\"\"\n        correct_tokens = 0\n        total_tokens = 0\n        \n        for pred, ref in zip(predictions, references):\n            pred_tokens = self.tgt_vocab.tokenize(pred)\n            ref_tokens = self.tgt_vocab.tokenize(ref)\n            \n            # Compare tokens up to the length of prediction\n            min_len = min(len(pred_tokens), len(ref_tokens))\n            for i in range(min_len):\n                if pred_tokens[i] == ref_tokens[i]:\n                    correct_tokens += 1\n                total_tokens += 1\n            \n            # Add penalty for length mismatch\n            total_tokens += abs(len(pred_tokens) - len(ref_tokens))\n        \n        return correct_tokens / total_tokens if total_tokens > 0 else 0\n    \n    def calculate_exact_match(self, predictions, references):\n        \"\"\"Calculate exact match accuracy\"\"\"\n        exact_matches = 0\n        \n        for pred, ref in zip(predictions, references):\n            # Normalize whitespace for comparison\n            pred_norm = ' '.join(pred.split())\n            ref_norm = ' '.join(ref.split())\n            \n            if pred_norm == ref_norm:\n                exact_matches += 1\n        \n        return exact_matches / len(predictions) if predictions else 0\n    \n    def calculate_bleu(self, predictions, references):\n        \"\"\"Calculate BLEU score\"\"\"\n        # Format references as list of lists (sacrebleu format)\n        refs = [[ref] for ref in references]\n        \n        try:\n            bleu_score = self.bleu.corpus_score(predictions, list(zip(*refs)))\n            return bleu_score.score\n        except Exception as e:\n            print(f\"Error calculating BLEU: {e}\")\n            return 0.0\n    \n    def analyze_by_length(self, predictions, references, src_texts):\n        \"\"\"Analyze performance by source sequence length\"\"\"\n        length_buckets = defaultdict(lambda: {'predictions': [], 'references': []})\n        \n        for pred, ref, src in zip(predictions, references, src_texts):\n            src_len = len(self.src_vocab.tokenize(src))\n            \n            # Categorize by length\n            if src_len <= 10:\n                bucket = '0-10'\n            elif src_len <= 20:\n                bucket = '11-20'\n            elif src_len <= 30:\n                bucket = '21-30'\n            else:\n                bucket = '31+'\n            \n            length_buckets[bucket]['predictions'].append(pred)\n            length_buckets[bucket]['references'].append(ref)\n        \n        # Calculate metrics for each bucket\n        results = {}\n        for bucket, data in sorted(length_buckets.items()):\n            if data['predictions']:\n                bleu = self.calculate_bleu(data['predictions'], data['references'])\n                token_acc = self.calculate_token_accuracy(data['predictions'], data['references'])\n                exact_match = self.calculate_exact_match(data['predictions'], data['references'])\n                \n                results[bucket] = {\n                    'count': len(data['predictions']),\n                    'bleu': bleu,\n                    'token_accuracy': token_acc,\n                    'exact_match': exact_match\n                }\n        \n        return results\n    \n    def evaluate(self, data_loader):\n        \"\"\"Full evaluation\"\"\"\n        print(f'\\nEvaluating {self.model_name}...')\n        \n        # Generate predictions\n        predictions, references, src_texts = self.generate_sequences(data_loader)\n        \n        # Calculate overall metrics\n        bleu_score = self.calculate_bleu(predictions, references)\n        token_accuracy = self.calculate_token_accuracy(predictions, references)\n        exact_match = self.calculate_exact_match(predictions, references)\n        \n        print(f'\\nOverall Results:')\n        print(f'  BLEU Score:        {bleu_score:.2f}')\n        print(f'  Token Accuracy:    {token_accuracy*100:.2f}%')\n        print(f'  Exact Match:       {exact_match*100:.2f}%')\n        \n        # Analyze by length\n        length_analysis = self.analyze_by_length(predictions, references, src_texts)\n        \n        print(f'\\nResults by Source Length:')\n        for bucket, metrics in sorted(length_analysis.items()):\n            print(f'  Length {bucket}: (n={metrics[\"count\"]})')\n            print(f'    BLEU:          {metrics[\"bleu\"]:.2f}')\n            print(f'    Token Acc:     {metrics[\"token_accuracy\"]*100:.2f}%')\n            print(f'    Exact Match:   {metrics[\"exact_match\"]*100:.2f}%')\n        \n        # Save results\n        results = {\n            'model': self.model_name,\n            'overall': {\n                'bleu': float(bleu_score),\n                'token_accuracy': float(token_accuracy),\n                'exact_match': float(exact_match)\n            },\n            'by_length': {k: {mk: float(mv) for mk, mv in v.items()} \n                         for k, v in length_analysis.items()},\n            'examples': self.get_example_predictions(predictions, references, src_texts, n=10)\n        }\n        \n        # Save to file\n        results_dir = os.path.join(self.config['paths']['results'], self.model_name)\n        os.makedirs(results_dir, exist_ok=True)\n        \n        with open(os.path.join(results_dir, 'evaluation_results.json'), 'w') as f:\n            json.dump(results, f, indent=2)\n        \n        print(f'\\nResults saved to {results_dir}')\n        \n        return results\n    \n    def get_example_predictions(self, predictions, references, src_texts, n=10):\n        \"\"\"Get example predictions for analysis\"\"\"\n        examples = []\n        \n        indices = np.random.choice(len(predictions), min(n, len(predictions)), replace=False)\n        \n        for idx in indices:\n            examples.append({\n                'source': src_texts[idx],\n                'reference': references[idx],\n                'prediction': predictions[idx]\n            })\n        \n        return examples\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Evaluate Seq2Seq models')\n    parser.add_argument('--config', type=str, default='config.yaml',\n                        help='Path to config file')\n    parser.add_argument('--model', type=str, default='all',\n                        choices=['vanilla', 'lstm', 'attention', 'all'],\n                        help='Which model to evaluate')\n    parser.add_argument('--split', type=str, default='test',\n                        choices=['val', 'test'],\n                        help='Which split to evaluate on')\n    args = parser.parse_args()\n    \n    # Load config\n    with open(args.config, 'r') as f:\n        config = yaml.safe_load(f)\n    \n    # Set device\n    device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    \n    # Vocabularies will be loaded from checkpoints\n    src_vocab = None\n    tgt_vocab = None\n    \n    # Determine which models to evaluate\n    models_to_eval = []\n    if args.model == 'all':\n        models_to_eval = ['vanilla', 'lstm', 'attention']\n    else:\n        models_to_eval = [args.model]\n    \n    all_results = {}\n    \n    # Evaluate models\n    for model_name in models_to_eval:\n        print(f'\\n{\"=\"*60}')\n        print(f'Evaluating {model_name.upper()} model')\n        print(f'{\"=\"*60}')\n        \n        # Load checkpoint first to get the config it was trained with\n        checkpoint_path = os.path.join(\n            config['paths']['checkpoints'],\n            model_name,\n            'best_model.pt'\n        )\n        \n        if not os.path.exists(checkpoint_path):\n            print(f'Checkpoint not found: {checkpoint_path}')\n            continue\n        \n        # Load checkpoint to get training config and vocabularies\n        print(f'Loading checkpoint: {checkpoint_path}')\n        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n        \n        # Extract vocabularies from checkpoint\n        if 'src_vocab' in checkpoint and 'tgt_vocab' in checkpoint:\n            src_vocab = checkpoint['src_vocab']\n            tgt_vocab = checkpoint['tgt_vocab']\n            print(f'Loaded vocabularies from checkpoint: src={src_vocab.n_words}, tgt={tgt_vocab.n_words}')\n        else:\n            print('Error: Vocabularies not found in checkpoint!')\n            print('Please retrain the model with the updated train.py')\n            continue\n        \n        # Use the config from checkpoint (has correct model dimensions)\n        model_config = checkpoint.get('config', config)\n        print(f'Using embedding_dim={model_config[\"model\"][\"embedding_dim\"]}, hidden_dim={model_config[\"model\"][\"hidden_dim\"]}')\n        \n        src_vocab_size = src_vocab.n_words\n        tgt_vocab_size = tgt_vocab.n_words\n        \n        # Create model with checkpoint's config and vocab sizes\n        if model_name == 'vanilla':\n            model = create_vanilla_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n        elif model_name == 'lstm':\n            model = create_lstm_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n        elif model_name == 'attention':\n            model = create_attention_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n        \n        # Load the state dict\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}')\n        print(f'Validation loss: {checkpoint[\"val_loss\"]:.4f}')\n        \n        # Load evaluation data with checkpoint's vocabularies (only for first model)\n        if 'data_loader' not in locals():\n            print('Loading evaluation data...')\n            from datasets import load_dataset\n            from data_loader import CodeSearchNetDataset, collate_fn\n            from torch.utils.data import DataLoader\n            \n            dataset = load_dataset(\n                model_config['dataset']['name'],\n                split='train',\n                cache_dir=model_config['dataset']['cache_dir']\n            )\n            \n            total_needed = model_config['dataset']['train_size'] + model_config['dataset']['val_size'] + model_config['dataset']['test_size']\n            dataset = dataset.shuffle(seed=42).select(range(min(total_needed, len(dataset))))\n            \n            splits = dataset.train_test_split(test_size=model_config['dataset']['val_size'] + model_config['dataset']['test_size'], seed=42)\n            temp_splits = splits['test'].train_test_split(test_size=model_config['dataset']['test_size'], seed=42)\n            \n            eval_data = temp_splits['test'] if args.split == 'test' else temp_splits['train']\n            max_src_len = model_config['dataset'].get('max_docstring_length', 50)\n            max_tgt_len = model_config['dataset'].get('max_code_length', 100)\n            eval_dataset = CodeSearchNetDataset(eval_data, src_vocab, tgt_vocab, max_src_len, max_tgt_len)\n            data_loader = DataLoader(\n                eval_dataset,\n                batch_size=model_config['training']['batch_size'],\n                shuffle=False,\n                collate_fn=collate_fn\n            )\n            print(f'Evaluation dataset size: {len(eval_dataset)}')\n        \n        # Create evaluator and evaluate\n        evaluator = Evaluator(model, model_config, device, model_name, src_vocab, tgt_vocab)\n        results = evaluator.evaluate(data_loader)\n        \n        all_results[model_name] = {\n            'overall': results['overall'],\n            'by_length': results['by_length']\n        }\n    \n    # Print comparison\n    if len(all_results) > 1:\n        # Overall comparison\n        print(f'\\n{\"=\"*60}')\n        print('Model Comparison — Overall')\n        print(f'{\"=\"*60}')\n        print(f'{\"Model\":<15} {\"BLEU\":<10} {\"Token Acc\":<12} {\"Exact Match\":<12}')\n        print('-' * 60)\n        for model_name, data in all_results.items():\n            metrics = data['overall']\n            print(f'{model_name:<15} {metrics[\"bleu\"]:<10.2f} '\n                  f'{metrics[\"token_accuracy\"]*100:<12.2f} '\n                  f'{metrics[\"exact_match\"]*100:<12.2f}')\n\n        # Cross-model by-length comparison\n        all_buckets = sorted(set(\n            bucket\n            for data in all_results.values()\n            for bucket in data['by_length'].keys()\n        ))\n\n        for metric_key, metric_label in [\n            ('bleu', 'BLEU'),\n            ('token_accuracy', 'Token Accuracy (%)'),\n            ('exact_match', 'Exact Match (%)'),\n        ]:\n            print(f'\\n{\"=\"*60}')\n            print(f'Model Comparison by Docstring Length — {metric_label}')\n            print(f'{\"=\"*60}')\n            model_names = list(all_results.keys())\n            header = f'{\"Length\":<10}' + ''.join(f'{m:<15}' for m in model_names)\n            print(header)\n            print('-' * (10 + 15 * len(model_names)))\n            for bucket in all_buckets:\n                row = f'{bucket:<10}'\n                for model_name in model_names:\n                    bucket_data = all_results[model_name]['by_length'].get(bucket)\n                    if bucket_data:\n                        val = bucket_data[metric_key]\n                        if metric_key != 'bleu':\n                            val *= 100\n                        row += f'{val:<15.2f}'\n                    else:\n                        row += f'{\"N/A\":<15}'\n                print(row)\n\n\nif __name__ == '__main__':\n    main()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dd98c5de",
   "metadata": {},
   "source": "### visualize_attention.py"
  },
  {
   "cell_type": "code",
   "id": "645d6151",
   "metadata": {},
   "source": "%%writefile visualize_attention.py\n\"\"\"\nAttention visualization script\n\"\"\"\nimport matplotlib\nmatplotlib.use('Agg')  # Use non-interactive backend\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport yaml\nimport os\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom data_loader import load_and_prepare_data, Vocabulary\nfrom models import create_attention_seq2seq\n\n\nclass AttentionVisualizer:\n    \"\"\"Visualize attention weights\"\"\"\n    \n    def __init__(self, model, config, device, src_vocab, tgt_vocab):\n        self.model = model\n        self.config = config\n        self.device = device\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        \n        # Visualization directory\n        self.viz_dir = os.path.join(config['paths']['visualizations'], 'attention')\n        os.makedirs(self.viz_dir, exist_ok=True)\n    \n    def load_checkpoint(self, checkpoint_path):\n        \"\"\"Load model checkpoint\"\"\"\n        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}')\n    \n    def visualize_attention(self, src_tokens, tgt_tokens, attention_weights, example_id):\n        \"\"\"\n        Visualize attention weights as a heatmap\n        \n        Args:\n            src_tokens: list of source tokens\n            tgt_tokens: list of target tokens\n            attention_weights: (tgt_len, src_len) attention matrix\n            example_id: identifier for saving the plot\n        \"\"\"\n        try:\n            # Limit visualization to reasonable size\n            max_tgt_tokens = 50\n            max_src_tokens = 30\n            \n            if len(tgt_tokens) > max_tgt_tokens:\n                tgt_tokens = tgt_tokens[:max_tgt_tokens]\n                attention_weights = attention_weights[:max_tgt_tokens, :]\n                print(f'  Note: Truncated visualization to first {max_tgt_tokens} target tokens')\n            \n            if len(src_tokens) > max_src_tokens:\n                src_tokens = src_tokens[:max_src_tokens]\n                attention_weights = attention_weights[:, :max_src_tokens]\n                print(f'  Note: Truncated visualization to first {max_src_tokens} source tokens')\n            \n            # Create figure\n            fig, ax = plt.subplots(figsize=(max(10, len(src_tokens) * 0.5), \n                                           max(8, len(tgt_tokens) * 0.4)))\n            \n            # Create heatmap\n            sns.heatmap(\n                attention_weights,\n                xticklabels=src_tokens,\n                yticklabels=tgt_tokens,\n                cmap='YlOrRd',\n                cbar=True,\n                ax=ax,\n                vmin=0,\n                vmax=1,\n                square=False\n            )\n            \n            ax.set_xlabel('Source Sequence (Docstring)', fontsize=12, fontweight='bold')\n            ax.set_ylabel('Target Sequence (Generated Code)', fontsize=12, fontweight='bold')\n            ax.set_title(f'Attention Weights - Example {example_id}', fontsize=14, fontweight='bold')\n            \n            # Rotate labels for better readability\n            plt.xticks(rotation=45, ha='right', fontsize=8)\n            plt.yticks(rotation=0, fontsize=8)\n            \n            plt.tight_layout()\n            \n            # Save figure\n            save_path = os.path.join(self.viz_dir, f'attention_example_{example_id}.png')\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            plt.close()\n            \n            print(f'Saved attention visualization: {save_path}')\n        except Exception as e:\n            print(f'Error creating visualization: {e}')\n            plt.close('all')\n    \n    def visualize_example(self, src, tgt, src_text, tgt_text, example_id):\n        \"\"\"Visualize attention for a single example\"\"\"\n        self.model.eval()\n        \n        with torch.no_grad():\n            src = src.unsqueeze(0).to(self.device)\n            src_lengths = torch.tensor([src.shape[1]])\n            \n            # Generate with attention\n            max_length = self.config['dataset']['max_code_length'] + 2\n            generated, attention_weights = self.model.generate(\n                src, src_lengths, max_length, self.tgt_vocab.SOS_token\n            )\n            \n            # Get generated sequence\n            generated_indices = generated[0].cpu().tolist()\n            generated_text = self.tgt_vocab.decode(generated_indices)\n            \n            # Get attention weights (remove batch dimension)\n            attention_weights = attention_weights[0].cpu().numpy()\n            \n            # Get tokens\n            src_tokens = self.src_vocab.tokenize(src_text)\n            tgt_tokens = self.tgt_vocab.tokenize(generated_text)\n            \n            # Truncate attention to actual lengths\n            attention_weights = attention_weights[:len(tgt_tokens), :len(src_tokens)]\n            \n            # Print example\n            print(f'\\n{\"=\"*80}')\n            print(f'Example {example_id}')\n            print(f'{\"=\"*80}')\n            print(f'Source (Docstring):\\n  {src_text}')\n            print(f'\\nReference Code:\\n  {tgt_text}')\n            print(f'\\nGenerated Code:\\n  {generated_text}')\n            print(f'\\nAttention matrix shape: {attention_weights.shape}')\n            \n            # Visualize attention\n            self.visualize_attention(src_tokens, tgt_tokens, attention_weights, example_id)\n            \n            # Save detailed info\n            info = {\n                'source': src_text,\n                'reference': tgt_text,\n                'generated': generated_text,\n                'src_tokens': src_tokens,\n                'tgt_tokens': tgt_tokens,\n                'attention_shape': attention_weights.shape\n            }\n            \n            return info, attention_weights\n    \n    def analyze_attention_patterns(self, attention_weights, src_tokens, tgt_tokens):\n        \"\"\"Analyze attention patterns\"\"\"\n        print(f'\\nAttention Analysis:')\n        \n        # Find max attention for each target token\n        max_attentions = np.max(attention_weights, axis=1)\n        max_src_indices = np.argmax(attention_weights, axis=1)\n        \n        print(f'\\nTarget -> Most Attended Source:')\n        for i, (tgt_token, src_idx, max_att) in enumerate(zip(tgt_tokens, max_src_indices, max_attentions)):\n            if src_idx < len(src_tokens):\n                print(f'  {tgt_token:<15} -> {src_tokens[src_idx]:<15} (weight: {max_att:.3f})')\n        \n        # Calculate attention entropy (measure of focus)\n        epsilon = 1e-10\n        entropy = -np.sum(attention_weights * np.log(attention_weights + epsilon), axis=1)\n        avg_entropy = np.mean(entropy)\n        \n        print(f'\\nAverage Attention Entropy: {avg_entropy:.3f}')\n        print(f'  (Lower entropy = more focused attention)')\n    \n    def visualize_multiple_examples(self, data_loader, num_examples=5):\n        \"\"\"Visualize attention for multiple examples\"\"\"\n        print(f'\\nVisualizing attention for {num_examples} examples...')\n        \n        examples_found = 0\n        \n        with torch.no_grad():\n            for batch in data_loader:\n                if examples_found >= num_examples:\n                    break\n                \n                src = batch['src']\n                tgt = batch['tgt']\n                src_texts = batch['src_texts']\n                tgt_texts = batch['tgt_texts']\n                \n                for i in range(src.shape[0]):\n                    if examples_found >= num_examples:\n                        break\n                    \n                    info, attention_weights = self.visualize_example(\n                        src[i],\n                        tgt[i],\n                        src_texts[i],\n                        tgt_texts[i],\n                        examples_found + 1\n                    )\n                    \n                    # Analyze attention patterns\n                    self.analyze_attention_patterns(\n                        attention_weights,\n                        info['src_tokens'],\n                        info['tgt_tokens']\n                    )\n                    \n                    examples_found += 1\n        \n        print(f'\\n{\"=\"*80}')\n        print(f'Visualized {examples_found} examples')\n        print(f'Visualizations saved to: {self.viz_dir}')\n        print(f'{\"=\"*80}')\n    \n    def create_summary_visualization(self, data_loader, num_samples=100):\n        \"\"\"Create summary visualization of attention statistics\"\"\"\n        print(f'\\nCreating attention summary visualization...')\n        \n        all_entropies = []\n        all_max_attentions = []\n        \n        self.model.eval()\n        \n        samples_processed = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(data_loader, desc='Processing'):\n                if samples_processed >= num_samples:\n                    break\n                \n                src = batch['src'].to(self.device)\n                src_lengths = batch['src_lengths']\n                \n                max_length = self.config['dataset']['max_code_length'] + 2\n                _, attention_weights = self.model.generate(\n                    src, src_lengths, max_length, self.tgt_vocab.SOS_token\n                )\n                \n                # Calculate statistics\n                for i in range(attention_weights.shape[0]):\n                    if samples_processed >= num_samples:\n                        break\n                    \n                    att = attention_weights[i].cpu().numpy()\n                    \n                    # Entropy\n                    epsilon = 1e-10\n                    entropy = -np.sum(att * np.log(att + epsilon), axis=1)\n                    all_entropies.extend(entropy.tolist())\n                    \n                    # Max attention\n                    max_att = np.max(att, axis=1)\n                    all_max_attentions.extend(max_att.tolist())\n                    \n                    samples_processed += 1\n        \n        # Create summary plot\n        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Entropy distribution\n        axes[0].hist(all_entropies, bins=50, edgecolor='black', alpha=0.7)\n        axes[0].set_xlabel('Attention Entropy', fontsize=12)\n        axes[0].set_ylabel('Frequency', fontsize=12)\n        axes[0].set_title('Distribution of Attention Entropy', fontsize=14, fontweight='bold')\n        axes[0].axvline(np.mean(all_entropies), color='red', linestyle='--', \n                       label=f'Mean: {np.mean(all_entropies):.3f}')\n        axes[0].legend()\n        \n        # Max attention distribution\n        axes[1].hist(all_max_attentions, bins=50, edgecolor='black', alpha=0.7, color='orange')\n        axes[1].set_xlabel('Maximum Attention Weight', fontsize=12)\n        axes[1].set_ylabel('Frequency', fontsize=12)\n        axes[1].set_title('Distribution of Maximum Attention Weights', fontsize=14, fontweight='bold')\n        axes[1].axvline(np.mean(all_max_attentions), color='red', linestyle='--',\n                       label=f'Mean: {np.mean(all_max_attentions):.3f}')\n        axes[1].legend()\n        \n        plt.tight_layout()\n        \n        save_path = os.path.join(self.viz_dir, 'attention_statistics.png')\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        print(f'Saved attention statistics: {save_path}')\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Visualize attention weights')\n    parser.add_argument('--config', type=str, default='config.yaml',\n                        help='Path to config file')\n    parser.add_argument('--num_examples', type=int, default=5,\n                        help='Number of examples to visualize')\n    parser.add_argument('--summary', action='store_true',\n                        help='Create summary statistics visualization')\n    args = parser.parse_args()\n    \n    # Load config\n    with open(args.config, 'r') as f:\n        config = yaml.safe_load(f)\n    \n    # Set device\n    device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    \n    # Load checkpoint first to check if it exists\n    checkpoint_path = os.path.join(\n        config['paths']['checkpoints'],\n        'attention',\n        'best_model.pt'\n    )\n    \n    if not os.path.exists(checkpoint_path):\n        print(f'Error: Checkpoint not found: {checkpoint_path}')\n        print('Please train the attention model first.')\n        print('\\nTo train the model, run:')\n        print('  python train.py --config config_quick.yaml --model attention')\n        return\n    \n    # Load checkpoint to get vocabularies and config\n    print('Loading checkpoint...')\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    \n    # Extract vocabularies from checkpoint\n    if 'src_vocab' not in checkpoint or 'tgt_vocab' not in checkpoint:\n        print('Error: Vocabularies not found in checkpoint!')\n        print('Please retrain the model with the updated train.py')\n        return\n    \n    src_vocab = checkpoint['src_vocab']\n    tgt_vocab = checkpoint['tgt_vocab']\n    print(f'Loaded vocabularies from checkpoint: src={src_vocab.n_words}, tgt={tgt_vocab.n_words}')\n    \n    # Use config from checkpoint (has correct dimensions)\n    model_config = checkpoint.get('config', config)\n    print(f'Using embedding_dim={model_config[\"model\"][\"embedding_dim\"]}, hidden_dim={model_config[\"model\"][\"hidden_dim\"]}')\n    \n    # Load test data with checkpoint's vocabularies\n    print('Loading test data...')\n    from datasets import load_dataset\n    from data_loader import CodeSearchNetDataset, collate_fn\n    from torch.utils.data import DataLoader\n    \n    dataset = load_dataset(\n        model_config['dataset']['name'],\n        split='train',\n        cache_dir=model_config['dataset']['cache_dir']\n    )\n    \n    total_needed = model_config['dataset']['train_size'] + model_config['dataset']['val_size'] + model_config['dataset']['test_size']\n    dataset = dataset.shuffle(seed=42).select(range(min(total_needed, len(dataset))))\n    \n    splits = dataset.train_test_split(test_size=model_config['dataset']['val_size'] + model_config['dataset']['test_size'], seed=42)\n    temp_splits = splits['test'].train_test_split(test_size=model_config['dataset']['test_size'], seed=42)\n    test_data = temp_splits['test']\n    \n    max_src_len = model_config['dataset'].get('max_docstring_length', 50)\n    max_tgt_len = model_config['dataset'].get('max_code_length', 100)\n    test_dataset = CodeSearchNetDataset(test_data, src_vocab, tgt_vocab, max_src_len, max_tgt_len)\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=model_config['training']['batch_size'],\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    print(f'Test dataset size: {len(test_dataset)}')\n    \n    src_vocab_size = src_vocab.n_words\n    tgt_vocab_size = tgt_vocab.n_words\n    \n    # Create attention model with checkpoint's config and vocab sizes\n    print('Creating attention model...')\n    model = create_attention_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n    \n    # Load checkpoint weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}, val_loss={checkpoint[\"val_loss\"]:.4f}')\n    \n    # Create visualizer\n    visualizer = AttentionVisualizer(model, model_config, device, src_vocab, tgt_vocab)\n    \n    # Visualize examples\n    visualizer.visualize_multiple_examples(test_loader, num_examples=args.num_examples)\n    \n    # Create summary visualization\n    if args.summary:\n        num_samples = min(100, len(test_dataset))\n        visualizer.create_summary_visualization(test_loader, num_samples=num_samples)\n\n\nif __name__ == '__main__':\n    main()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71085e94",
   "metadata": {},
   "source": "### generate_report_plots.py"
  },
  {
   "cell_type": "code",
   "id": "0e87bed5",
   "metadata": {},
   "source": "%%writefile generate_report_plots.py\n\"\"\"\nGenerate plots and figures for the report\n\"\"\"\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport argparse\nimport numpy as np\n\nsns.set_style('whitegrid')\nsns.set_palette('husl')\n\n\ndef plot_training_curves(model_names, config):\n    \"\"\"Plot training and validation loss curves\"\"\"\n    fig, axes = plt.subplots(1, len(model_names), figsize=(6*len(model_names), 5))\n    \n    if len(model_names) == 1:\n        axes = [axes]\n    \n    for idx, model_name in enumerate(model_names):\n        # Load training history\n        history_path = os.path.join(config['paths']['logs'], model_name, 'training_history.json')\n        \n        if not os.path.exists(history_path):\n            print(f\"Warning: History not found for {model_name}\")\n            continue\n        \n        with open(history_path, 'r') as f:\n            history = json.load(f)\n        \n        epochs = range(1, len(history['train_losses']) + 1)\n        \n        axes[idx].plot(epochs, history['train_losses'], label='Train Loss', linewidth=2)\n        axes[idx].plot(epochs, history['val_losses'], label='Val Loss', linewidth=2)\n        axes[idx].set_xlabel('Epoch', fontsize=12)\n        axes[idx].set_ylabel('Loss', fontsize=12)\n        axes[idx].set_title(f'{model_name.upper()} - Training Progress', \n                           fontsize=14, fontweight='bold')\n        axes[idx].legend()\n        axes[idx].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Save\n    save_path = os.path.join(config['paths']['results'], 'training_curves.png')\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"Saved training curves: {save_path}\")\n    plt.close()\n\n\ndef plot_model_comparison(model_names, config):\n    \"\"\"Plot comparison of models across metrics\"\"\"\n    metrics_data = {\n        'model': [],\n        'BLEU': [],\n        'Token Accuracy': [],\n        'Exact Match': []\n    }\n    \n    for model_name in model_names:\n        results_path = os.path.join(config['paths']['results'], model_name, 'evaluation_results.json')\n        \n        if not os.path.exists(results_path):\n            print(f\"Warning: Results not found for {model_name}\")\n            continue\n        \n        with open(results_path, 'r') as f:\n            results = json.load(f)\n        \n        metrics_data['model'].append(model_name.upper())\n        metrics_data['BLEU'].append(results['overall']['bleu'])\n        metrics_data['Token Accuracy'].append(results['overall']['token_accuracy'] * 100)\n        metrics_data['Exact Match'].append(results['overall']['exact_match'] * 100)\n    \n    # Create grouped bar chart\n    x = np.arange(len(metrics_data['model']))\n    width = 0.25\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    rects1 = ax.bar(x - width, metrics_data['BLEU'], width, label='BLEU Score')\n    rects2 = ax.bar(x, metrics_data['Token Accuracy'], width, label='Token Accuracy (%)')\n    rects3 = ax.bar(x + width, metrics_data['Exact Match'], width, label='Exact Match (%)')\n    \n    ax.set_xlabel('Model', fontsize=14, fontweight='bold')\n    ax.set_ylabel('Score', fontsize=14, fontweight='bold')\n    ax.set_title('Model Comparison Across Metrics', fontsize=16, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(metrics_data['model'])\n    ax.legend(fontsize=12)\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels on bars\n    def autolabel(rects):\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate(f'{height:.1f}',\n                       xy=(rect.get_x() + rect.get_width() / 2, height),\n                       xytext=(0, 3),\n                       textcoords=\"offset points\",\n                       ha='center', va='bottom',\n                       fontsize=10)\n    \n    autolabel(rects1)\n    autolabel(rects2)\n    autolabel(rects3)\n    \n    plt.tight_layout()\n    \n    # Save\n    save_path = os.path.join(config['paths']['results'], 'model_comparison.png')\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"Saved model comparison: {save_path}\")\n    plt.close()\n\n\ndef plot_performance_by_length(model_names, config):\n    \"\"\"Plot performance by input length\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    metric_names = ['bleu', 'token_accuracy', 'exact_match']\n    titles = ['BLEU Score by Length', 'Token Accuracy by Length', 'Exact Match by Length']\n    \n    for metric_idx, (metric, title) in enumerate(zip(metric_names, titles)):\n        for model_name in model_names:\n            results_path = os.path.join(config['paths']['results'], model_name, \n                                       'evaluation_results.json')\n            \n            if not os.path.exists(results_path):\n                continue\n            \n            with open(results_path, 'r') as f:\n                results = json.load(f)\n            \n            # Extract length buckets\n            buckets = sorted(results['by_length'].keys())\n            values = []\n            \n            for bucket in buckets:\n                val = results['by_length'][bucket][metric]\n                if metric != 'bleu':\n                    val *= 100\n                values.append(val)\n            \n            axes[metric_idx].plot(buckets, values, marker='o', linewidth=2, \n                                 label=model_name.upper(), markersize=8)\n        \n        axes[metric_idx].set_xlabel('Source Length (tokens)', fontsize=12)\n        axes[metric_idx].set_ylabel('Score', fontsize=12)\n        axes[metric_idx].set_title(title, fontsize=14, fontweight='bold')\n        axes[metric_idx].legend()\n        axes[metric_idx].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Save\n    save_path = os.path.join(config['paths']['results'], 'performance_by_length.png')\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"Saved performance by length: {save_path}\")\n    plt.close()\n\n\ndef generate_results_table(model_names, config):\n    \"\"\"Generate LaTeX table of results\"\"\"\n    table = \"\\\\begin{table}[h]\\n\"\n    table += \"\\\\centering\\n\"\n    table += \"\\\\caption{Model Performance Comparison}\\n\"\n    table += \"\\\\begin{tabular}{|l|c|c|c|}\\n\"\n    table += \"\\\\hline\\n\"\n    table += \"\\\\textbf{Model} & \\\\textbf{BLEU} & \\\\textbf{Token Acc (\\\\%)} & \\\\textbf{Exact Match (\\\\%)} \\\\\\\\\\n\"\n    table += \"\\\\hline\\n\"\n    \n    for model_name in model_names:\n        results_path = os.path.join(config['paths']['results'], model_name, \n                                   'evaluation_results.json')\n        \n        if not os.path.exists(results_path):\n            continue\n        \n        with open(results_path, 'r') as f:\n            results = json.load(f)\n        \n        bleu = results['overall']['bleu']\n        token_acc = results['overall']['token_accuracy'] * 100\n        exact_match = results['overall']['exact_match'] * 100\n        \n        table += f\"{model_name.upper()} & {bleu:.2f} & {token_acc:.2f} & {exact_match:.2f} \\\\\\\\\\n\"\n    \n    table += \"\\\\hline\\n\"\n    table += \"\\\\end{tabular}\\n\"\n    table += \"\\\\end{table}\\n\"\n    \n    # Save\n    save_path = os.path.join(config['paths']['results'], 'results_table.tex')\n    with open(save_path, 'w') as f:\n        f.write(table)\n    \n    print(f\"Saved LaTeX table: {save_path}\")\n    print(\"\\nLaTeX table:\")\n    print(table)\n\n\ndef main():\n    import yaml\n    \n    parser = argparse.ArgumentParser(description='Generate plots for report')\n    parser.add_argument('--config', type=str, default='config.yaml')\n    args = parser.parse_args()\n    \n    # Load config\n    with open(args.config, 'r') as f:\n        config = yaml.safe_load(f)\n    \n    model_names = ['vanilla', 'lstm', 'attention']\n    \n    print(\"Generating plots for report...\")\n    print(\"=\"*60)\n    \n    # Generate all plots\n    plot_training_curves(model_names, config)\n    plot_model_comparison(model_names, config)\n    plot_performance_by_length(model_names, config)\n    generate_results_table(model_names, config)\n    \n    print(\"=\"*60)\n    print(\"All plots generated successfully!\")\n    print(f\"Check the '{config['paths']['results']}' directory\")\n\n\nif __name__ == '__main__':\n    main()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56167a1a",
   "metadata": {},
   "source": "## Step 4 — Create Config\nEdit hyperparameters here, then re-run."
  },
  {
   "cell_type": "code",
   "id": "73f78914",
   "metadata": {},
   "source": "%%writefile config_colab.yaml\ndataset:\n  name: \"Nan-Do/code-search-net-python\"\n  train_size: 10000\n  val_size: 1000\n  test_size: 1000\n  max_docstring_length: 50\n  max_code_length: 80\n  cache_dir: \"./data_cache\"\n\nmodel:\n  embedding_dim: 256\n  hidden_dim: 256\n  dropout: 0.3\n  max_vocab_size: 10000\n\ntraining:\n  batch_size: 64\n  num_epochs: 5\n  learning_rate: 0.001\n  teacher_forcing_ratio: 0.5\n  gradient_clip: 5.0\n  save_every: 1\n\npaths:\n  checkpoints: \"./checkpoints\"\n  results: \"./results\"\n  visualizations: \"./visualizations\"\n  logs: \"./logs\"\n\ndevice: \"cuda\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ac57e8c",
   "metadata": {},
   "source": "## Step 5 — Train All 3 Models\nTakes ~30–60 minutes on T4 GPU. Progress bars show loss per epoch."
  },
  {
   "cell_type": "code",
   "id": "60761cc0",
   "metadata": {},
   "source": "!python train.py --config config_colab.yaml --model all",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "618e7bb2",
   "metadata": {},
   "source": "## Step 6 — Evaluate All Models\nComputes BLEU, Token Accuracy, Exact Match.\nAlso prints a cross-model comparison table broken down by docstring length."
  },
  {
   "cell_type": "code",
   "id": "c6723942",
   "metadata": {},
   "source": "!python evaluate.py --config config_colab.yaml --model all --split test",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "79da1bc8",
   "metadata": {},
   "source": "## Step 7 — Attention Heatmaps & Report Plots"
  },
  {
   "cell_type": "code",
   "id": "8242ffa9",
   "metadata": {},
   "source": "# Attention heatmaps (5 examples + entropy stats)\n!python visualize_attention.py --config config_colab.yaml --num_examples 5 --summary\n\n# All report plots (training curves, model comparison, performance-by-length)\n!python generate_report_plots.py --config config_colab.yaml\n\n# Display heatmaps inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os\n\nviz_dir = \"./visualizations/attention\"\npng_files = sorted([f for f in os.listdir(viz_dir) if f.endswith(\".png\")])\nprint(f\"Found {len(png_files)} attention visualizations\")\n\nfig, axes = plt.subplots(1, min(5, len(png_files)), figsize=(25, 6))\nif len(png_files) == 1:\n    axes = [axes]\nfor i, fname in enumerate(png_files[:5]):\n    img = mpimg.imread(os.path.join(viz_dir, fname))\n    axes[i].imshow(img)\n    axes[i].set_title(fname.replace(\".png\", \"\"), fontsize=9)\n    axes[i].axis(\"off\")\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bebdf864",
   "metadata": {},
   "source": "## Step 8 — View Report Plots Inline"
  },
  {
   "cell_type": "code",
   "id": "b59b3bea",
   "metadata": {},
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os\n\nplots = [\n    (\"./results/training_curves.png\",       \"Training Curves\"),\n    (\"./results/model_comparison.png\",       \"Model Comparison\"),\n    (\"./results/performance_by_length.png\",  \"Performance by Docstring Length\"),\n]\n\nfor fpath, title in plots:\n    if os.path.exists(fpath):\n        img = mpimg.imread(fpath)\n        plt.figure(figsize=(14, 6))\n        plt.imshow(img)\n        plt.title(title, fontsize=14)\n        plt.axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(f\"Not found: {fpath}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e05c349",
   "metadata": {},
   "source": "## Step 9 — Download All Results\nDownloads three zip files: trained model weights, evaluation results, and visualizations."
  },
  {
   "cell_type": "code",
   "id": "4157ef2a",
   "metadata": {},
   "source": "import zipfile, os\nfrom google.colab import files\n\ndef zip_folder(folder, zipname):\n    with zipfile.ZipFile(zipname, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        for root, dirs, fnames in os.walk(folder):\n            for fname in fnames:\n                zf.write(os.path.join(root, fname))\n    print(f\"Zipped: {zipname} ({os.path.getsize(zipname)/1e6:.1f} MB)\")\n\nzip_folder(\"./checkpoints\",   \"checkpoints.zip\")\nzip_folder(\"./results\",       \"results.zip\")\nzip_folder(\"./visualizations\",\"visualizations.zip\")\n\nfiles.download(\"checkpoints.zip\")\nfiles.download(\"results.zip\")\nfiles.download(\"visualizations.zip\")\nprint(\"Done! Check your Downloads folder.\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}