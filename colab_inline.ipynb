{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e99e454",
   "metadata": {},
   "source": [
    "# Seq2Seq Code Generation — Google Colab (Inline Edition)\n",
    "### All source code lives directly in this notebook — edit any cell and re-run to update that file\n",
    "**Before running:** `Runtime → Change runtime type → GPU (T4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26256ef",
   "metadata": {},
   "source": [
    "## Step 1 — Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets sacrebleu seaborn pyyaml tqdm -q\n",
    "print(\"All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d7edb",
   "metadata": {},
   "source": [
    "## Step 2 — Check GPU\n",
    "If you see \"No GPU\", go to `Runtime → Change runtime type → T4 GPU` and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfa619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"Training will be fast!\")\n",
    "else:\n",
    "    print(\"No GPU — go to Runtime → Change runtime type → T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a35c23",
   "metadata": {},
   "source": [
    "## Step 3 — Source Files\n",
    "Each cell below writes one file to disk.\n",
    "**Edit the code here in Colab, then re-run the cell to save your changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "print(\"models/ directory ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161acda",
   "metadata": {},
   "source": [
    "### models/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/__init__.py\n",
    "\"\"\"\n",
    "Models package initialization\n",
    "\"\"\"\n",
    "from .vanilla_rnn import create_vanilla_seq2seq\n",
    "from .lstm import create_lstm_seq2seq\n",
    "from .attention_lstm import create_attention_seq2seq\n",
    "\n",
    "__all__ = [\n",
    "    'create_vanilla_seq2seq',\n",
    "    'create_lstm_seq2seq',\n",
    "    'create_attention_seq2seq'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca560d1",
   "metadata": {},
   "source": [
    "### models/vanilla_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/vanilla_rnn.py\n",
    "\"\"\"\n",
    "Vanilla RNN-based Seq2Seq Model\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Vanilla RNN Encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_seq, input_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_seq: (batch_size, seq_len)\n",
    "            input_lengths: (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, seq_len, hidden_dim)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        \n",
    "        # Unpack\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"Vanilla RNN Decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_token: (batch_size, 1)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, output_size)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        output = self.out(output.squeeze(1))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class VanillaSeq2Seq(nn.Module):\n",
    "    \"\"\"Vanilla RNN Seq2Seq Model\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(VanillaSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            src_lengths: (batch_size,)\n",
    "            tgt: (batch_size, tgt_len)\n",
    "            teacher_forcing_ratio: probability of using teacher forcing\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, tgt_len, output_size)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        _, hidden = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # First input to decoder is SOS token\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, src, src_lengths, max_length, sos_token):\n",
    "        \"\"\"\n",
    "        Generate output sequence\n",
    "        \n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            src_lengths: (batch_size,)\n",
    "            max_length: maximum length of generated sequence\n",
    "            sos_token: start of sequence token\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, max_length)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # Encode\n",
    "        _, hidden = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # Start with SOS token\n",
    "        input_token = torch.tensor([[sos_token]] * batch_size).to(self.device)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            top1 = output.argmax(1)\n",
    "            outputs.append(top1.unsqueeze(1))\n",
    "            input_token = top1.unsqueeze(1)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def create_vanilla_seq2seq(src_vocab_size, tgt_vocab_size, config, device):\n",
    "    \"\"\"Create Vanilla RNN Seq2Seq model\"\"\"\n",
    "    encoder = EncoderRNN(\n",
    "        src_vocab_size,\n",
    "        config['model']['embedding_dim'],\n",
    "        config['model']['hidden_dim'],\n",
    "        config['model']['dropout']\n",
    "    )\n",
    "    \n",
    "    decoder = DecoderRNN(\n",
    "        tgt_vocab_size,\n",
    "        config['model']['embedding_dim'],\n",
    "        config['model']['hidden_dim'],\n",
    "        config['model']['dropout']\n",
    "    )\n",
    "    \n",
    "    model = VanillaSeq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    config = {\n",
    "        'model': {\n",
    "            'embedding_dim': 256,\n",
    "            'hidden_dim': 256,\n",
    "            'dropout': 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model = create_vanilla_seq2seq(5000, 5000, config, device)\n",
    "    \n",
    "    # Test forward pass\n",
    "    src = torch.randint(0, 5000, (32, 20)).to(device)\n",
    "    tgt = torch.randint(0, 5000, (32, 30)).to(device)\n",
    "    src_lengths = torch.tensor([20] * 32)\n",
    "    \n",
    "    outputs = model(src, src_lengths, tgt)\n",
    "    print(\"Output shape:\", outputs.shape)\n",
    "    \n",
    "    # Test generation\n",
    "    generated = model.generate(src, src_lengths, 30, 1)\n",
    "    print(\"Generated shape:\", generated.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412f115",
   "metadata": {},
   "source": [
    "### models/lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd575897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/lstm.py\n",
    "\"\"\"\n",
    "LSTM-based Seq2Seq Model\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    \"\"\"LSTM Encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_seq, input_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_seq: (batch_size, seq_len)\n",
    "            input_lengths: (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, seq_len, hidden_dim)\n",
    "            hidden: tuple of (h_n, c_n) each (1, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"LSTM Decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_token: (batch_size, 1)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "            cell: (1, batch_size, hidden_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, output_size)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "            cell: (1, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        output = self.out(output.squeeze(1))\n",
    "        \n",
    "        return output, hidden, cell\n",
    "\n",
    "\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    \"\"\"LSTM Seq2Seq Model\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            src_lengths: (batch_size,)\n",
    "            tgt: (batch_size, tgt_len)\n",
    "            teacher_forcing_ratio: probability of using teacher forcing\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, tgt_len, output_size)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        _, (hidden, cell) = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # First input to decoder is SOS token\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, src, src_lengths, max_length, sos_token):\n",
    "        \"\"\"\n",
    "        Generate output sequence\n",
    "        \n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            src_lengths: (batch_size,)\n",
    "            max_length: maximum length of generated sequence\n",
    "            sos_token: start of sequence token\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, max_length)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # Encode\n",
    "        _, (hidden, cell) = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # Start with SOS token\n",
    "        input_token = torch.tensor([[sos_token]] * batch_size).to(self.device)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            outputs.append(top1.unsqueeze(1))\n",
    "            input_token = top1.unsqueeze(1)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "def create_lstm_seq2seq(src_vocab_size, tgt_vocab_size, config, device):\n",
    "    \"\"\"Create LSTM Seq2Seq model\"\"\"\n",
    "    encoder = EncoderLSTM(\n",
    "        src_vocab_size,\n",
    "        config['model']['embedding_dim'],\n",
    "        config['model']['hidden_dim'],\n",
    "        config['model']['dropout']\n",
    "    )\n",
    "    \n",
    "    decoder = DecoderLSTM(\n",
    "        tgt_vocab_size,\n",
    "        config['model']['embedding_dim'],\n",
    "        config['model']['hidden_dim'],\n",
    "        config['model']['dropout']\n",
    "    )\n",
    "    \n",
    "    model = LSTMSeq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    config = {\n",
    "        'model': {\n",
    "            'embedding_dim': 256,\n",
    "            'hidden_dim': 256,\n",
    "            'dropout': 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model = create_lstm_seq2seq(5000, 5000, config, device)\n",
    "    \n",
    "    # Test forward pass\n",
    "    src = torch.randint(0, 5000, (32, 20)).to(device)\n",
    "    tgt = torch.randint(0, 5000, (32, 30)).to(device)\n",
    "    src_lengths = torch.tensor([20] * 32)\n",
    "    \n",
    "    outputs = model(src, src_lengths, tgt)\n",
    "    print(\"Output shape:\", outputs.shape)\n",
    "    \n",
    "    # Test generation\n",
    "    generated = model.generate(src, src_lengths, 30, 1)\n",
    "    print(\"Generated shape:\", generated.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148e445",
   "metadata": {},
   "source": [
    "### models/attention_lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/attention_lstm.py\n",
    "\"\"\"\n",
    "LSTM with Bahdanau Attention Seq2Seq Model\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "\n",
    "class EncoderBiLSTM(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM Encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(EncoderBiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear layer to project bidirectional hidden state to decoder hidden size\n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_cell = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "    \n",
    "    def forward(self, input_seq, input_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_seq: (batch_size, seq_len)\n",
    "            input_lengths: (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, seq_len, hidden_dim * 2)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "            cell: (1, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, input_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # hidden and cell are (2, batch_size, hidden_dim) for bidirectional\n",
    "        # Concatenate forward and backward and project to decoder size\n",
    "        hidden = torch.tanh(self.fc_hidden(torch.cat((hidden[0], hidden[1]), dim=1))).unsqueeze(0)\n",
    "        cell = torch.tanh(self.fc_cell(torch.cat((cell[0], cell[1]), dim=1))).unsqueeze(0)\n",
    "        \n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Bahdanau (Additive) Attention Mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, encoder_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        \n",
    "        # Attention layers\n",
    "        self.attn_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attn_encoder = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden: (batch_size, hidden_dim) - decoder hidden state\n",
    "            encoder_outputs: (batch_size, src_len, encoder_dim) - all encoder outputs\n",
    "            mask: (batch_size, src_len) - mask for padding\n",
    "        \n",
    "        Returns:\n",
    "            context: (batch_size, encoder_dim) - weighted context vector\n",
    "            attention_weights: (batch_size, src_len) - attention weights\n",
    "        \"\"\"\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # (batch_size, src_len, hidden_dim)\n",
    "        \n",
    "        # Calculate attention energy\n",
    "        energy = torch.tanh(\n",
    "            self.attn_hidden(hidden) + self.attn_encoder(encoder_outputs)\n",
    "        )  # (batch_size, src_len, hidden_dim)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention = self.v(energy).squeeze(2)  # (batch_size, src_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention, dim=1)  # (batch_size, src_len)\n",
    "        \n",
    "        # Calculate context vector\n",
    "        context = torch.bmm(\n",
    "            attention_weights.unsqueeze(1),\n",
    "            encoder_outputs\n",
    "        ).squeeze(1)  # (batch_size, encoder_dim)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class AttentionDecoderLSTM(nn.Module):\n",
    "    \"\"\"LSTM Decoder with Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, encoder_dim, dropout=0.3):\n",
    "        super(AttentionDecoderLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim, padding_idx=0)\n",
    "        self.attention = BahdanauAttention(hidden_dim, encoder_dim)\n",
    "        \n",
    "        # LSTM input is embedding + context vector\n",
    "        self.lstm = nn.LSTM(embedding_dim + encoder_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(hidden_dim + encoder_dim + embedding_dim, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_token: (batch_size, 1)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "            cell: (1, batch_size, hidden_dim)\n",
    "            encoder_outputs: (batch_size, src_len, encoder_dim)\n",
    "            mask: (batch_size, src_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, output_size)\n",
    "            hidden: (1, batch_size, hidden_dim)\n",
    "            cell: (1, batch_size, hidden_dim)\n",
    "            attention_weights: (batch_size, src_len)\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input_token))  # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # Calculate attention\n",
    "        context, attention_weights = self.attention(\n",
    "            hidden.squeeze(0),\n",
    "            encoder_outputs,\n",
    "            mask\n",
    "        )  # context: (batch_size, encoder_dim)\n",
    "        \n",
    "        # Concatenate embedding and context\n",
    "        lstm_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)  # (batch_size, 1, embedding_dim + encoder_dim)\n",
    "        \n",
    "        # LSTM forward\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        \n",
    "        # Concatenate output, context, and embedding for final prediction\n",
    "        output = output.squeeze(1)  # (batch_size, hidden_dim)\n",
    "        embedded = embedded.squeeze(1)  # (batch_size, embedding_dim)\n",
    "        \n",
    "        pred_input = torch.cat((output, context, embedded), dim=1)  # (batch_size, hidden_dim + encoder_dim + embedding_dim)\n",
    "        prediction = self.out(pred_input)  # (batch_size, output_size)\n",
    "        \n",
    "        return prediction, hidden, cell, attention_weights\n",
    "\n",
    "\n",
    "class AttentionSeq2Seq(nn.Module):\n",
    "    \"\"\"LSTM with Attention Seq2Seq Model\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(AttentionSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def create_mask(self, src, src_lengths):\n",
    "        \"\"\"Create mask for padding\"\"\"\n",
    "        mask = torch.zeros_like(src, dtype=torch.bool)\n",
    "        for i, length in enumerate(src_lengths):\n",
    "            mask[i, :length] = 1\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            src_lengths: (batch_size,)\n",
    "            tgt: (batch_size, tgt_len)\n",
    "            teacher_forcing_ratio: probability of using teacher forcing\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, tgt_len, output_size)\n",
    "            attentions: (batch_size, tgt_len, src_len)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_size\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        # Tensors to store decoder outputs and attention weights\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        attentions = torch.zeros(batch_size, tgt_len, src_len).to(self.device)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = self.create_mask(src, src_lengths).to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # First input to decoder is SOS token\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, attention_weights = self.decoder(\n",
    "                input_token, hidden, cell, encoder_outputs, mask\n",
    "            )\n",
    "            outputs[:, t] = output\n",
    "            attentions[:, t] = attention_weights\n",
    "            \n",
    "            # Teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs, attentions\n",
    "    \n",
    "    def generate(self, src, src_lengths, max_length, sos_token):\n",
    "        \"\"\"\n",
    "        Generate output sequence\n",
    "        \n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            src_lengths: (batch_size,)\n",
    "            max_length: maximum length of generated sequence\n",
    "            sos_token: start of sequence token\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch_size, max_length)\n",
    "            attentions: (batch_size, max_length, src_len)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        # Create mask\n",
    "        mask = self.create_mask(src, src_lengths).to(self.device)\n",
    "        \n",
    "        # Encode\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # Start with SOS token\n",
    "        input_token = torch.tensor([[sos_token]] * batch_size).to(self.device)\n",
    "        \n",
    "        outputs = []\n",
    "        attentions = torch.zeros(batch_size, max_length, src_len).to(self.device)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            output, hidden, cell, attention_weights = self.decoder(\n",
    "                input_token, hidden, cell, encoder_outputs, mask\n",
    "            )\n",
    "            attentions[:, t] = attention_weights\n",
    "            top1 = output.argmax(1)\n",
    "            outputs.append(top1.unsqueeze(1))\n",
    "            input_token = top1.unsqueeze(1)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1), attentions\n",
    "\n",
    "\n",
    "def create_attention_seq2seq(src_vocab_size, tgt_vocab_size, config, device):\n",
    "    \"\"\"Create LSTM with Attention Seq2Seq model\"\"\"\n",
    "    encoder = EncoderBiLSTM(\n",
    "        src_vocab_size,\n",
    "        config['model']['embedding_dim'],\n",
    "        config['model']['hidden_dim'],\n",
    "        config['model']['dropout']\n",
    "    )\n",
    "    \n",
    "    decoder = AttentionDecoderLSTM(\n",
    "        tgt_vocab_size,\n",
    "        config['model']['embedding_dim'],\n",
    "        config['model']['hidden_dim'],\n",
    "        config['model']['hidden_dim'] * 2,  # bidirectional encoder\n",
    "        config['model']['dropout']\n",
    "    )\n",
    "    \n",
    "    model = AttentionSeq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    config = {\n",
    "        'model': {\n",
    "            'embedding_dim': 256,\n",
    "            'hidden_dim': 256,\n",
    "            'dropout': 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model = create_attention_seq2seq(5000, 5000, config, device)\n",
    "    \n",
    "    # Test forward pass\n",
    "    src = torch.randint(0, 5000, (32, 20)).to(device)\n",
    "    tgt = torch.randint(0, 5000, (32, 30)).to(device)\n",
    "    src_lengths = torch.tensor([20] * 32)\n",
    "    \n",
    "    outputs, attentions = model(src, src_lengths, tgt)\n",
    "    print(\"Output shape:\", outputs.shape)\n",
    "    print(\"Attention shape:\", attentions.shape)\n",
    "    \n",
    "    # Test generation\n",
    "    generated, gen_attentions = model.generate(src, src_lengths, 30, 1)\n",
    "    print(\"Generated shape:\", generated.shape)\n",
    "    print(\"Generated attention shape:\", gen_attentions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e3662",
   "metadata": {},
   "source": [
    "### data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_loader.py\n",
    "\"\"\"\n",
    "Data loading and preprocessing for CodeSearchNet dataset\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Builds and manages vocabulary for source and target sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_counts = Counter()\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_token = 0\n",
    "        self.SOS_token = 1\n",
    "        self.EOS_token = 2\n",
    "        self.UNK_token = 3\n",
    "        \n",
    "        self.word2idx['<PAD>'] = self.PAD_token\n",
    "        self.word2idx['<SOS>'] = self.SOS_token\n",
    "        self.word2idx['<EOS>'] = self.EOS_token\n",
    "        self.word2idx['<UNK>'] = self.UNK_token\n",
    "        \n",
    "        self.idx2word[self.PAD_token] = '<PAD>'\n",
    "        self.idx2word[self.SOS_token] = '<SOS>'\n",
    "        self.idx2word[self.EOS_token] = '<EOS>'\n",
    "        self.idx2word[self.UNK_token] = '<UNK>'\n",
    "        \n",
    "        self.n_words = 4\n",
    "    \n",
    "    def add_sentence(self, sentence: str):\n",
    "        \"\"\"Add all words in a sentence to vocabulary\"\"\"\n",
    "        for word in self.tokenize(sentence):\n",
    "            self.word_counts[word] += 1\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Build vocabulary from word counts, keeping most common words\"\"\"\n",
    "        # Get most common words\n",
    "        most_common = self.word_counts.most_common(self.max_vocab_size - 4)\n",
    "        \n",
    "        for word, _ in most_common:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.n_words\n",
    "                self.idx2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        \"\"\"Simple whitespace tokenization with some preprocessing\"\"\"\n",
    "        # Basic preprocessing\n",
    "        text = text.lower().strip()\n",
    "        # Split on whitespace and basic punctuation\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, sentence: str) -> List[int]:\n",
    "        \"\"\"Convert sentence to list of indices\"\"\"\n",
    "        tokens = self.tokenize(sentence)\n",
    "        return [self.word2idx.get(token, self.UNK_token) for token in tokens]\n",
    "    \n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"Convert list of indices back to sentence\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx == self.EOS_token:\n",
    "                break\n",
    "            if idx not in [self.PAD_token, self.SOS_token]:\n",
    "                words.append(self.idx2word.get(idx, '<UNK>'))\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save vocabulary to file\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'word2idx': self.word2idx,\n",
    "                'idx2word': self.idx2word,\n",
    "                'word_counts': self.word_counts,\n",
    "                'max_vocab_size': self.max_vocab_size,\n",
    "                'n_words': self.n_words\n",
    "            }, f)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load vocabulary from file\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.word2idx = data['word2idx']\n",
    "            self.idx2word = data['idx2word']\n",
    "            self.word_counts = data['word_counts']\n",
    "            self.max_vocab_size = data['max_vocab_size']\n",
    "            self.n_words = data['n_words']\n",
    "\n",
    "\n",
    "class CodeSearchNetDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for CodeSearchNet data\"\"\"\n",
    "    \n",
    "    def __init__(self, data, src_vocab, tgt_vocab, max_src_len, max_tgt_len):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Encode source (docstring)\n",
    "        src_indices = self.src_vocab.encode(item['docstring'])\n",
    "        # Truncate if too long\n",
    "        src_indices = src_indices[:self.max_src_len]\n",
    "        # Add EOS token\n",
    "        src_indices.append(self.src_vocab.EOS_token)\n",
    "        \n",
    "        # Encode target (code)\n",
    "        tgt_indices = self.tgt_vocab.encode(item['code'])\n",
    "        # Truncate if too long\n",
    "        tgt_indices = tgt_indices[:self.max_tgt_len]\n",
    "        # Add SOS and EOS tokens\n",
    "        tgt_indices = [self.tgt_vocab.SOS_token] + tgt_indices + [self.tgt_vocab.EOS_token]\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_indices, dtype=torch.long),\n",
    "            'src_text': item['docstring'],\n",
    "            'tgt_text': item['code']\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to pad sequences in a batch\"\"\"\n",
    "    src_batch = [item['src'] for item in batch]\n",
    "    tgt_batch = [item['tgt'] for item in batch]\n",
    "    src_texts = [item['src_text'] for item in batch]\n",
    "    tgt_texts = [item['tgt_text'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_lengths = torch.tensor([len(s) for s in src_batch])\n",
    "    tgt_lengths = torch.tensor([len(t) for t in tgt_batch])\n",
    "    \n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'src': src_padded,\n",
    "        'tgt': tgt_padded,\n",
    "        'src_lengths': src_lengths,\n",
    "        'tgt_lengths': tgt_lengths,\n",
    "        'src_texts': src_texts,\n",
    "        'tgt_texts': tgt_texts\n",
    "    }\n",
    "\n",
    "\n",
    "def load_and_prepare_data(config):\n",
    "    \"\"\"\n",
    "    Load CodeSearchNet dataset and prepare vocabularies\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader, src_vocab, tgt_vocab)\n",
    "    \"\"\"\n",
    "    print(\"Loading CodeSearchNet dataset...\")\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(\n",
    "        config['dataset']['name'],\n",
    "        split='train',\n",
    "        cache_dir=config['dataset']['cache_dir']\n",
    "    )\n",
    "    \n",
    "    print(f\"Total dataset size: {len(dataset)}\")\n",
    "    \n",
    "    # Take larger subset initially (we'll filter and then select what we need)\n",
    "    total_needed = config['dataset']['train_size'] + config['dataset']['val_size'] + config['dataset']['test_size']\n",
    "    # Get 5x more than needed to account for filtering\n",
    "    initial_sample = min(total_needed * 5, len(dataset))\n",
    "    dataset = dataset.shuffle(seed=42).select(range(initial_sample))\n",
    "    \n",
    "    # Filter out examples that are too long or empty\n",
    "    def filter_fn(example):\n",
    "        try:\n",
    "            # Handle different possible field names\n",
    "            doc = example.get('func_documentation_string') or example.get('docstring') or example.get('doc')\n",
    "            code = example.get('func_code_string') or example.get('code') or example.get('function')\n",
    "            \n",
    "            if not doc or not code:\n",
    "                return False\n",
    "            \n",
    "            # Check if strings are not empty after stripping\n",
    "            doc = str(doc).strip()\n",
    "            code = str(code).strip()\n",
    "            \n",
    "            if not doc or not code:\n",
    "                return False\n",
    "            \n",
    "            doc_len = len(Vocabulary.tokenize(doc))\n",
    "            code_len = len(Vocabulary.tokenize(code))\n",
    "            \n",
    "            return (doc_len > 2 and doc_len <= config['dataset']['max_docstring_length'] and\n",
    "                    code_len > 2 and code_len <= config['dataset']['max_code_length'])\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    print(\"Filtering dataset...\")\n",
    "    dataset = dataset.filter(filter_fn)\n",
    "    print(f\"After filtering: {len(dataset)} examples\")\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No examples passed filtering! The dataset might have different field names or all examples were too long.\")\n",
    "    \n",
    "    # Convert to simpler format\n",
    "    processed_data = []\n",
    "    for item in dataset:\n",
    "        # Handle different possible field names\n",
    "        doc = item.get('func_documentation_string') or item.get('docstring') or item.get('doc')\n",
    "        code = item.get('func_code_string') or item.get('code') or item.get('function')\n",
    "        \n",
    "        processed_data.append({\n",
    "            'docstring': str(doc).strip(),\n",
    "            'code': str(code).strip()\n",
    "        })\n",
    "        \n",
    "        # Stop if we have enough examples\n",
    "        if len(processed_data) >= total_needed:\n",
    "            break\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    train_size = config['dataset']['train_size']\n",
    "    val_size = config['dataset']['val_size']\n",
    "    \n",
    "    train_data = processed_data[:train_size]\n",
    "    val_data = processed_data[train_size:train_size + val_size]\n",
    "    test_data = processed_data[train_size + val_size:train_size + val_size + config['dataset']['test_size']]\n",
    "    \n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(train_data) == 0:\n",
    "        raise ValueError(f\"No training data! Got {len(processed_data)} examples after filtering, but needed at least {train_size}. Try reducing the dataset sizes in config.yaml or increasing max_docstring_length/max_code_length.\")\n",
    "    \n",
    "    if len(train_data) < train_size:\n",
    "        print(f\"⚠️  Warning: Only got {len(train_data)} training examples (requested {train_size})\")\n",
    "        print(f\"   Continuing with available data...\")\n",
    "    \n",
    "    # Build vocabularies\n",
    "    print(\"Building vocabularies...\")\n",
    "    src_vocab = Vocabulary(max_vocab_size=config['model']['max_vocab_size'])\n",
    "    tgt_vocab = Vocabulary(max_vocab_size=config['model']['max_vocab_size'])\n",
    "    \n",
    "    # Add all sentences to vocabulary\n",
    "    for item in train_data:\n",
    "        src_vocab.add_sentence(item['docstring'])\n",
    "        tgt_vocab.add_sentence(item['code'])\n",
    "    \n",
    "    src_vocab.build_vocab()\n",
    "    tgt_vocab.build_vocab()\n",
    "    \n",
    "    print(f\"Source vocabulary size: {src_vocab.n_words}\")\n",
    "    print(f\"Target vocabulary size: {tgt_vocab.n_words}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CodeSearchNetDataset(\n",
    "        train_data, src_vocab, tgt_vocab,\n",
    "        config['dataset']['max_docstring_length'],\n",
    "        config['dataset']['max_code_length']\n",
    "    )\n",
    "    val_dataset = CodeSearchNetDataset(\n",
    "        val_data, src_vocab, tgt_vocab,\n",
    "        config['dataset']['max_docstring_length'],\n",
    "        config['dataset']['max_code_length']\n",
    "    )\n",
    "    test_dataset = CodeSearchNetDataset(\n",
    "        test_data, src_vocab, tgt_vocab,\n",
    "        config['dataset']['max_docstring_length'],\n",
    "        config['dataset']['max_code_length']\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Save vocabularies\n",
    "    os.makedirs(config['paths']['checkpoints'], exist_ok=True)\n",
    "    src_vocab.save(os.path.join(config['paths']['checkpoints'], 'src_vocab.pkl'))\n",
    "    tgt_vocab.save(os.path.join(config['paths']['checkpoints'], 'tgt_vocab.pkl'))\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, src_vocab, tgt_vocab\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import yaml\n",
    "    \n",
    "    # Test data loading\n",
    "    with open('config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    train_loader, val_loader, test_loader, src_vocab, tgt_vocab = load_and_prepare_data(config)\n",
    "    \n",
    "    # Print sample batch\n",
    "    for batch in train_loader:\n",
    "        print(\"Source shape:\", batch['src'].shape)\n",
    "        print(\"Target shape:\", batch['tgt'].shape)\n",
    "        print(\"Source text:\", batch['src_texts'][0])\n",
    "        print(\"Target text:\", batch['tgt_texts'][0])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102245ae",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d451540",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\"\"\"\n",
    "Training script for Seq2Seq models\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import load_and_prepare_data, Vocabulary\n",
    "from models import create_vanilla_seq2seq, create_lstm_seq2seq, create_attention_seq2seq\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer class for Seq2Seq models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, device, model_name, src_vocab, tgt_vocab):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "        self.optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['training']['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        # Create directories\n",
    "        self.checkpoint_dir = os.path.join(config['paths']['checkpoints'], model_name)\n",
    "        self.log_dir = os.path.join(config['paths']['logs'], model_name)\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "    \n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            src = batch['src'].to(self.device)\n",
    "            tgt = batch['tgt'].to(self.device)\n",
    "            src_lengths = batch['src_lengths']\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            if 'attention' in self.model_name:\n",
    "                outputs, _ = self.model(\n",
    "                    src, src_lengths, tgt,\n",
    "                    teacher_forcing_ratio=self.config['training']['teacher_forcing_ratio']\n",
    "                )\n",
    "            else:\n",
    "                outputs = self.model(\n",
    "                    src, src_lengths, tgt,\n",
    "                    teacher_forcing_ratio=self.config['training']['teacher_forcing_ratio']\n",
    "                )\n",
    "            \n",
    "            # Calculate loss\n",
    "            # Reshape: outputs (batch_size, tgt_len, vocab_size) -> (batch_size * tgt_len, vocab_size)\n",
    "            # tgt (batch_size, tgt_len) -> (batch_size * tgt_len)\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:].contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = self.criterion(outputs, tgt)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                self.config['training']['gradient_clip']\n",
    "            )\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        return epoch_loss / len(train_loader)\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        \"\"\"Evaluate on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                src = batch['src'].to(self.device)\n",
    "                tgt = batch['tgt'].to(self.device)\n",
    "                src_lengths = batch['src_lengths']\n",
    "                \n",
    "                # Forward pass\n",
    "                if 'attention' in self.model_name:\n",
    "                    outputs, _ = self.model(src, src_lengths, tgt, teacher_forcing_ratio=0)\n",
    "                else:\n",
    "                    outputs = self.model(src, src_lengths, tgt, teacher_forcing_ratio=0)\n",
    "                \n",
    "                # Calculate loss\n",
    "                output_dim = outputs.shape[-1]\n",
    "                outputs = outputs[:, 1:].contiguous().view(-1, output_dim)\n",
    "                tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                loss = self.criterion(outputs, tgt)\n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "        return epoch_loss / len(val_loader)\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"\\nTraining {self.model_name}...\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch(train_loader, epoch)\n",
    "            val_loss = self.evaluate(val_loader)\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f}')\n",
    "            print(f'  Val Loss:   {val_loss:.4f}')\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % self.config['training']['save_every'] == 0:\n",
    "                self.save_checkpoint(epoch, val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch, val_loss, is_best=True)\n",
    "                print(f'  New best model saved!')\n",
    "        \n",
    "        # Save training history\n",
    "        self.save_training_history()\n",
    "        \n",
    "        print(f'\\nTraining completed for {self.model_name}!')\n",
    "        print(f'Best validation loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_loss, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'config': self.config,\n",
    "            'src_vocab': self.src_vocab,\n",
    "            'tgt_vocab': self.tgt_vocab\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "        else:\n",
    "            path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "        \n",
    "        torch.save(checkpoint, path)\n",
    "    \n",
    "    def save_training_history(self):\n",
    "        \"\"\"Save training history\"\"\"\n",
    "        history = {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(self.log_dir, 'training_history.json')\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Train Seq2Seq models')\n",
    "    parser.add_argument('--config', type=str, default='config.yaml',\n",
    "                        help='Path to config file')\n",
    "    parser.add_argument('--model', type=str, default='all',\n",
    "                        choices=['vanilla', 'lstm', 'attention', 'all'],\n",
    "                        help='Which model to train')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load config\n",
    "    with open(args.config, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Load data\n",
    "    print('Loading data...')\n",
    "    train_loader, val_loader, test_loader, src_vocab, tgt_vocab = load_and_prepare_data(config)\n",
    "    \n",
    "    src_vocab_size = src_vocab.n_words\n",
    "    tgt_vocab_size = tgt_vocab.n_words\n",
    "    \n",
    "    print(f'Source vocabulary size: {src_vocab_size}')\n",
    "    print(f'Target vocabulary size: {tgt_vocab_size}')\n",
    "    \n",
    "    # Determine which models to train\n",
    "    models_to_train = []\n",
    "    if args.model == 'all':\n",
    "        models_to_train = ['vanilla', 'lstm', 'attention']\n",
    "    else:\n",
    "        models_to_train = [args.model]\n",
    "    \n",
    "    # Train models\n",
    "    for model_name in models_to_train:\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'Training {model_name.upper()} model')\n",
    "        print(f'{\"=\"*60}')\n",
    "        \n",
    "        # Create model\n",
    "        if model_name == 'vanilla':\n",
    "            model = create_vanilla_seq2seq(src_vocab_size, tgt_vocab_size, config, device)\n",
    "        elif model_name == 'lstm':\n",
    "            model = create_lstm_seq2seq(src_vocab_size, tgt_vocab_size, config, device)\n",
    "        elif model_name == 'attention':\n",
    "            model = create_attention_seq2seq(src_vocab_size, tgt_vocab_size, config, device)\n",
    "        \n",
    "        # Create trainer and train\n",
    "        trainer = Trainer(model, config, device, model_name, src_vocab, tgt_vocab)\n",
    "        trainer.train(train_loader, val_loader, config['training']['num_epochs'])\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('All training completed!')\n",
    "    print('='*60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d17fad",
   "metadata": {},
   "source": [
    "### evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eeb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\"\"\"\n",
    "Evaluation metrics and testing script\n",
    "\"\"\"\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sacrebleu.metrics import BLEU\n",
    "from collections import defaultdict\n",
    "\n",
    "from data_loader import load_and_prepare_data, Vocabulary\n",
    "from models import create_vanilla_seq2seq, create_lstm_seq2seq, create_attention_seq2seq\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluation class for Seq2Seq models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, device, model_name, src_vocab, tgt_vocab):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        \n",
    "        # BLEU metric\n",
    "        self.bleu = BLEU()\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}')\n",
    "        print(f'Validation loss: {checkpoint[\"val_loss\"]:.4f}')\n",
    "    \n",
    "    def generate_sequences(self, data_loader):\n",
    "        \"\"\"Generate sequences for all examples in data loader\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_references = []\n",
    "        all_src_texts = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc='Generating'):\n",
    "                src = batch['src'].to(self.device)\n",
    "                src_lengths = batch['src_lengths']\n",
    "                src_texts = batch['src_texts']\n",
    "                tgt_texts = batch['tgt_texts']\n",
    "                \n",
    "                # Generate\n",
    "                max_length = self.config['dataset']['max_code_length'] + 2\n",
    "                if 'attention' in self.model_name:\n",
    "                    generated, _ = self.model.generate(\n",
    "                        src, src_lengths, max_length, self.tgt_vocab.SOS_token\n",
    "                    )\n",
    "                else:\n",
    "                    generated = self.model.generate(\n",
    "                        src, src_lengths, max_length, self.tgt_vocab.SOS_token\n",
    "                    )\n",
    "                \n",
    "                # Decode generated sequences\n",
    "                for i in range(generated.shape[0]):\n",
    "                    pred_indices = generated[i].cpu().tolist()\n",
    "                    pred_text = self.tgt_vocab.decode(pred_indices)\n",
    "                    \n",
    "                    all_predictions.append(pred_text)\n",
    "                    all_references.append(tgt_texts[i])\n",
    "                    all_src_texts.append(src_texts[i])\n",
    "        \n",
    "        return all_predictions, all_references, all_src_texts\n",
    "    \n",
    "    def calculate_token_accuracy(self, predictions, references):\n",
    "        \"\"\"Calculate token-level accuracy\"\"\"\n",
    "        correct_tokens = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_tokens = self.tgt_vocab.tokenize(pred)\n",
    "            ref_tokens = self.tgt_vocab.tokenize(ref)\n",
    "            \n",
    "            # Compare tokens up to the length of prediction\n",
    "            min_len = min(len(pred_tokens), len(ref_tokens))\n",
    "            for i in range(min_len):\n",
    "                if pred_tokens[i] == ref_tokens[i]:\n",
    "                    correct_tokens += 1\n",
    "                total_tokens += 1\n",
    "            \n",
    "            # Add penalty for length mismatch\n",
    "            total_tokens += abs(len(pred_tokens) - len(ref_tokens))\n",
    "        \n",
    "        return correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    def calculate_exact_match(self, predictions, references):\n",
    "        \"\"\"Calculate exact match accuracy\"\"\"\n",
    "        exact_matches = 0\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Normalize whitespace for comparison\n",
    "            pred_norm = ' '.join(pred.split())\n",
    "            ref_norm = ' '.join(ref.split())\n",
    "            \n",
    "            if pred_norm == ref_norm:\n",
    "                exact_matches += 1\n",
    "        \n",
    "        return exact_matches / len(predictions) if predictions else 0\n",
    "    \n",
    "    def calculate_bleu(self, predictions, references):\n",
    "        \"\"\"Calculate BLEU score\"\"\"\n",
    "        # Format references as list of lists (sacrebleu format)\n",
    "        refs = [[ref] for ref in references]\n",
    "        \n",
    "        try:\n",
    "            bleu_score = self.bleu.corpus_score(predictions, list(zip(*refs)))\n",
    "            return bleu_score.score\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating BLEU: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def analyze_by_length(self, predictions, references, src_texts):\n",
    "        \"\"\"Analyze performance by source sequence length\"\"\"\n",
    "        length_buckets = defaultdict(lambda: {'predictions': [], 'references': []})\n",
    "        \n",
    "        for pred, ref, src in zip(predictions, references, src_texts):\n",
    "            src_len = len(self.src_vocab.tokenize(src))\n",
    "            \n",
    "            # Categorize by length\n",
    "            if src_len <= 10:\n",
    "                bucket = '0-10'\n",
    "            elif src_len <= 20:\n",
    "                bucket = '11-20'\n",
    "            elif src_len <= 30:\n",
    "                bucket = '21-30'\n",
    "            else:\n",
    "                bucket = '31+'\n",
    "            \n",
    "            length_buckets[bucket]['predictions'].append(pred)\n",
    "            length_buckets[bucket]['references'].append(ref)\n",
    "        \n",
    "        # Calculate metrics for each bucket\n",
    "        results = {}\n",
    "        for bucket, data in sorted(length_buckets.items()):\n",
    "            if data['predictions']:\n",
    "                bleu = self.calculate_bleu(data['predictions'], data['references'])\n",
    "                token_acc = self.calculate_token_accuracy(data['predictions'], data['references'])\n",
    "                exact_match = self.calculate_exact_match(data['predictions'], data['references'])\n",
    "                \n",
    "                results[bucket] = {\n",
    "                    'count': len(data['predictions']),\n",
    "                    'bleu': bleu,\n",
    "                    'token_accuracy': token_acc,\n",
    "                    'exact_match': exact_match\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"Full evaluation\"\"\"\n",
    "        print(f'\\nEvaluating {self.model_name}...')\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions, references, src_texts = self.generate_sequences(data_loader)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        bleu_score = self.calculate_bleu(predictions, references)\n",
    "        token_accuracy = self.calculate_token_accuracy(predictions, references)\n",
    "        exact_match = self.calculate_exact_match(predictions, references)\n",
    "        \n",
    "        print(f'\\nOverall Results:')\n",
    "        print(f'  BLEU Score:        {bleu_score:.2f}')\n",
    "        print(f'  Token Accuracy:    {token_accuracy*100:.2f}%')\n",
    "        print(f'  Exact Match:       {exact_match*100:.2f}%')\n",
    "        \n",
    "        # Analyze by length\n",
    "        length_analysis = self.analyze_by_length(predictions, references, src_texts)\n",
    "        \n",
    "        print(f'\\nResults by Source Length:')\n",
    "        for bucket, metrics in sorted(length_analysis.items()):\n",
    "            print(f'  Length {bucket}: (n={metrics[\"count\"]})')\n",
    "            print(f'    BLEU:          {metrics[\"bleu\"]:.2f}')\n",
    "            print(f'    Token Acc:     {metrics[\"token_accuracy\"]*100:.2f}%')\n",
    "            print(f'    Exact Match:   {metrics[\"exact_match\"]*100:.2f}%')\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'model': self.model_name,\n",
    "            'overall': {\n",
    "                'bleu': float(bleu_score),\n",
    "                'token_accuracy': float(token_accuracy),\n",
    "                'exact_match': float(exact_match)\n",
    "            },\n",
    "            'by_length': {k: {mk: float(mv) for mk, mv in v.items()} \n",
    "                         for k, v in length_analysis.items()},\n",
    "            'examples': self.get_example_predictions(predictions, references, src_texts, n=10)\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        results_dir = os.path.join(self.config['paths']['results'], self.model_name)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        with open(os.path.join(results_dir, 'evaluation_results.json'), 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f'\\nResults saved to {results_dir}')\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_example_predictions(self, predictions, references, src_texts, n=10):\n",
    "        \"\"\"Get example predictions for analysis\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        indices = np.random.choice(len(predictions), min(n, len(predictions)), replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            examples.append({\n",
    "                'source': src_texts[idx],\n",
    "                'reference': references[idx],\n",
    "                'prediction': predictions[idx]\n",
    "            })\n",
    "        \n",
    "        return examples\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Evaluate Seq2Seq models')\n",
    "    parser.add_argument('--config', type=str, default='config.yaml',\n",
    "                        help='Path to config file')\n",
    "    parser.add_argument('--model', type=str, default='all',\n",
    "                        choices=['vanilla', 'lstm', 'attention', 'all'],\n",
    "                        help='Which model to evaluate')\n",
    "    parser.add_argument('--split', type=str, default='test',\n",
    "                        choices=['val', 'test'],\n",
    "                        help='Which split to evaluate on')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load config\n",
    "    with open(args.config, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Vocabularies will be loaded from checkpoints\n",
    "    src_vocab = None\n",
    "    tgt_vocab = None\n",
    "    \n",
    "    # Determine which models to evaluate\n",
    "    models_to_eval = []\n",
    "    if args.model == 'all':\n",
    "        models_to_eval = ['vanilla', 'lstm', 'attention']\n",
    "    else:\n",
    "        models_to_eval = [args.model]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # Evaluate models\n",
    "    for model_name in models_to_eval:\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'Evaluating {model_name.upper()} model')\n",
    "        print(f'{\"=\"*60}')\n",
    "        \n",
    "        # Load checkpoint first to get the config it was trained with\n",
    "        checkpoint_path = os.path.join(\n",
    "            config['paths']['checkpoints'],\n",
    "            model_name,\n",
    "            'best_model.pt'\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f'Checkpoint not found: {checkpoint_path}')\n",
    "            continue\n",
    "        \n",
    "        # Load checkpoint to get training config and vocabularies\n",
    "        print(f'Loading checkpoint: {checkpoint_path}')\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Extract vocabularies from checkpoint\n",
    "        if 'src_vocab' in checkpoint and 'tgt_vocab' in checkpoint:\n",
    "            src_vocab = checkpoint['src_vocab']\n",
    "            tgt_vocab = checkpoint['tgt_vocab']\n",
    "            print(f'Loaded vocabularies from checkpoint: src={src_vocab.n_words}, tgt={tgt_vocab.n_words}')\n",
    "        else:\n",
    "            print('Error: Vocabularies not found in checkpoint!')\n",
    "            print('Please retrain the model with the updated train.py')\n",
    "            continue\n",
    "        \n",
    "        # Use the config from checkpoint (has correct model dimensions)\n",
    "        model_config = checkpoint.get('config', config)\n",
    "        print(f'Using embedding_dim={model_config[\"model\"][\"embedding_dim\"]}, hidden_dim={model_config[\"model\"][\"hidden_dim\"]}')\n",
    "        \n",
    "        src_vocab_size = src_vocab.n_words\n",
    "        tgt_vocab_size = tgt_vocab.n_words\n",
    "        \n",
    "        # Create model with checkpoint's config and vocab sizes\n",
    "        if model_name == 'vanilla':\n",
    "            model = create_vanilla_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n",
    "        elif model_name == 'lstm':\n",
    "            model = create_lstm_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n",
    "        elif model_name == 'attention':\n",
    "            model = create_attention_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n",
    "        \n",
    "        # Load the state dict\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}')\n",
    "        print(f'Validation loss: {checkpoint[\"val_loss\"]:.4f}')\n",
    "        \n",
    "        # Load evaluation data with checkpoint's vocabularies (only for first model)\n",
    "        if 'data_loader' not in locals():\n",
    "            print('Loading evaluation data...')\n",
    "            from datasets import load_dataset\n",
    "            from data_loader import CodeSearchNetDataset, collate_fn\n",
    "            from torch.utils.data import DataLoader\n",
    "            \n",
    "            dataset = load_dataset(\n",
    "                model_config['dataset']['name'],\n",
    "                split='train',\n",
    "                cache_dir=model_config['dataset']['cache_dir']\n",
    "            )\n",
    "            \n",
    "            total_needed = model_config['dataset']['train_size'] + model_config['dataset']['val_size'] + model_config['dataset']['test_size']\n",
    "            dataset = dataset.shuffle(seed=42).select(range(min(total_needed, len(dataset))))\n",
    "            \n",
    "            splits = dataset.train_test_split(test_size=model_config['dataset']['val_size'] + model_config['dataset']['test_size'], seed=42)\n",
    "            temp_splits = splits['test'].train_test_split(test_size=model_config['dataset']['test_size'], seed=42)\n",
    "            \n",
    "            eval_data = temp_splits['test'] if args.split == 'test' else temp_splits['train']\n",
    "            max_src_len = model_config['dataset'].get('max_docstring_length', 50)\n",
    "            max_tgt_len = model_config['dataset'].get('max_code_length', 100)\n",
    "            eval_dataset = CodeSearchNetDataset(eval_data, src_vocab, tgt_vocab, max_src_len, max_tgt_len)\n",
    "            data_loader = DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=model_config['training']['batch_size'],\n",
    "                shuffle=False,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "            print(f'Evaluation dataset size: {len(eval_dataset)}')\n",
    "        \n",
    "        # Create evaluator and evaluate\n",
    "        evaluator = Evaluator(model, model_config, device, model_name, src_vocab, tgt_vocab)\n",
    "        results = evaluator.evaluate(data_loader)\n",
    "        \n",
    "        all_results[model_name] = {\n",
    "            'overall': results['overall'],\n",
    "            'by_length': results['by_length']\n",
    "        }\n",
    "    \n",
    "    # Print comparison\n",
    "    if len(all_results) > 1:\n",
    "        # Overall comparison\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print('Model Comparison — Overall')\n",
    "        print(f'{\"=\"*60}')\n",
    "        print(f'{\"Model\":<15} {\"BLEU\":<10} {\"Token Acc\":<12} {\"Exact Match\":<12}')\n",
    "        print('-' * 60)\n",
    "        for model_name, data in all_results.items():\n",
    "            metrics = data['overall']\n",
    "            print(f'{model_name:<15} {metrics[\"bleu\"]:<10.2f} '\n",
    "                  f'{metrics[\"token_accuracy\"]*100:<12.2f} '\n",
    "                  f'{metrics[\"exact_match\"]*100:<12.2f}')\n",
    "\n",
    "        # Cross-model by-length comparison\n",
    "        all_buckets = sorted(set(\n",
    "            bucket\n",
    "            for data in all_results.values()\n",
    "            for bucket in data['by_length'].keys()\n",
    "        ))\n",
    "\n",
    "        for metric_key, metric_label in [\n",
    "            ('bleu', 'BLEU'),\n",
    "            ('token_accuracy', 'Token Accuracy (%)'),\n",
    "            ('exact_match', 'Exact Match (%)'),\n",
    "        ]:\n",
    "            print(f'\\n{\"=\"*60}')\n",
    "            print(f'Model Comparison by Docstring Length — {metric_label}')\n",
    "            print(f'{\"=\"*60}')\n",
    "            model_names = list(all_results.keys())\n",
    "            header = f'{\"Length\":<10}' + ''.join(f'{m:<15}' for m in model_names)\n",
    "            print(header)\n",
    "            print('-' * (10 + 15 * len(model_names)))\n",
    "            for bucket in all_buckets:\n",
    "                row = f'{bucket:<10}'\n",
    "                for model_name in model_names:\n",
    "                    bucket_data = all_results[model_name]['by_length'].get(bucket)\n",
    "                    if bucket_data:\n",
    "                        val = bucket_data[metric_key]\n",
    "                        if metric_key != 'bleu':\n",
    "                            val *= 100\n",
    "                        row += f'{val:<15.2f}'\n",
    "                    else:\n",
    "                        row += f'{\"N/A\":<15}'\n",
    "                print(row)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98c5de",
   "metadata": {},
   "source": [
    "### visualize_attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile visualize_attention.py\n",
    "\"\"\"\n",
    "Attention visualization script\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_loader import load_and_prepare_data, Vocabulary\n",
    "from models import create_attention_seq2seq\n",
    "\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    \"\"\"Visualize attention weights\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, device, src_vocab, tgt_vocab):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        \n",
    "        # Visualization directory\n",
    "        self.viz_dir = os.path.join(config['paths']['visualizations'], 'attention')\n",
    "        os.makedirs(self.viz_dir, exist_ok=True)\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}')\n",
    "    \n",
    "    def visualize_attention(self, src_tokens, tgt_tokens, attention_weights, example_id):\n",
    "        \"\"\"\n",
    "        Visualize attention weights as a heatmap\n",
    "        \n",
    "        Args:\n",
    "            src_tokens: list of source tokens\n",
    "            tgt_tokens: list of target tokens\n",
    "            attention_weights: (tgt_len, src_len) attention matrix\n",
    "            example_id: identifier for saving the plot\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Limit visualization to reasonable size\n",
    "            max_tgt_tokens = 50\n",
    "            max_src_tokens = 30\n",
    "            \n",
    "            if len(tgt_tokens) > max_tgt_tokens:\n",
    "                tgt_tokens = tgt_tokens[:max_tgt_tokens]\n",
    "                attention_weights = attention_weights[:max_tgt_tokens, :]\n",
    "                print(f'  Note: Truncated visualization to first {max_tgt_tokens} target tokens')\n",
    "            \n",
    "            if len(src_tokens) > max_src_tokens:\n",
    "                src_tokens = src_tokens[:max_src_tokens]\n",
    "                attention_weights = attention_weights[:, :max_src_tokens]\n",
    "                print(f'  Note: Truncated visualization to first {max_src_tokens} source tokens')\n",
    "            \n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(max(10, len(src_tokens) * 0.5), \n",
    "                                           max(8, len(tgt_tokens) * 0.4)))\n",
    "            \n",
    "            # Create heatmap\n",
    "            sns.heatmap(\n",
    "                attention_weights,\n",
    "                xticklabels=src_tokens,\n",
    "                yticklabels=tgt_tokens,\n",
    "                cmap='YlOrRd',\n",
    "                cbar=True,\n",
    "                ax=ax,\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                square=False\n",
    "            )\n",
    "            \n",
    "            ax.set_xlabel('Source Sequence (Docstring)', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Target Sequence (Generated Code)', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(f'Attention Weights - Example {example_id}', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Rotate labels for better readability\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "            plt.yticks(rotation=0, fontsize=8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            save_path = os.path.join(self.viz_dir, f'attention_example_{example_id}.png')\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f'Saved attention visualization: {save_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Error creating visualization: {e}')\n",
    "            plt.close('all')\n",
    "    \n",
    "    def visualize_example(self, src, tgt, src_text, tgt_text, example_id):\n",
    "        \"\"\"Visualize attention for a single example\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            src = src.unsqueeze(0).to(self.device)\n",
    "            src_lengths = torch.tensor([src.shape[1]])\n",
    "            \n",
    "            # Generate with attention\n",
    "            max_length = self.config['dataset']['max_code_length'] + 2\n",
    "            generated, attention_weights = self.model.generate(\n",
    "                src, src_lengths, max_length, self.tgt_vocab.SOS_token\n",
    "            )\n",
    "            \n",
    "            # Get generated sequence\n",
    "            generated_indices = generated[0].cpu().tolist()\n",
    "            generated_text = self.tgt_vocab.decode(generated_indices)\n",
    "            \n",
    "            # Get attention weights (remove batch dimension)\n",
    "            attention_weights = attention_weights[0].cpu().numpy()\n",
    "            \n",
    "            # Get tokens\n",
    "            src_tokens = self.src_vocab.tokenize(src_text)\n",
    "            tgt_tokens = self.tgt_vocab.tokenize(generated_text)\n",
    "            \n",
    "            # Truncate attention to actual lengths\n",
    "            attention_weights = attention_weights[:len(tgt_tokens), :len(src_tokens)]\n",
    "            \n",
    "            # Print example\n",
    "            print(f'\\n{\"=\"*80}')\n",
    "            print(f'Example {example_id}')\n",
    "            print(f'{\"=\"*80}')\n",
    "            print(f'Source (Docstring):\\n  {src_text}')\n",
    "            print(f'\\nReference Code:\\n  {tgt_text}')\n",
    "            print(f'\\nGenerated Code:\\n  {generated_text}')\n",
    "            print(f'\\nAttention matrix shape: {attention_weights.shape}')\n",
    "            \n",
    "            # Visualize attention\n",
    "            self.visualize_attention(src_tokens, tgt_tokens, attention_weights, example_id)\n",
    "            \n",
    "            # Save detailed info\n",
    "            info = {\n",
    "                'source': src_text,\n",
    "                'reference': tgt_text,\n",
    "                'generated': generated_text,\n",
    "                'src_tokens': src_tokens,\n",
    "                'tgt_tokens': tgt_tokens,\n",
    "                'attention_shape': attention_weights.shape\n",
    "            }\n",
    "            \n",
    "            return info, attention_weights\n",
    "    \n",
    "    def analyze_attention_patterns(self, attention_weights, src_tokens, tgt_tokens):\n",
    "        \"\"\"Analyze attention patterns\"\"\"\n",
    "        print(f'\\nAttention Analysis:')\n",
    "        \n",
    "        # Find max attention for each target token\n",
    "        max_attentions = np.max(attention_weights, axis=1)\n",
    "        max_src_indices = np.argmax(attention_weights, axis=1)\n",
    "        \n",
    "        print(f'\\nTarget -> Most Attended Source:')\n",
    "        for i, (tgt_token, src_idx, max_att) in enumerate(zip(tgt_tokens, max_src_indices, max_attentions)):\n",
    "            if src_idx < len(src_tokens):\n",
    "                print(f'  {tgt_token:<15} -> {src_tokens[src_idx]:<15} (weight: {max_att:.3f})')\n",
    "        \n",
    "        # Calculate attention entropy (measure of focus)\n",
    "        epsilon = 1e-10\n",
    "        entropy = -np.sum(attention_weights * np.log(attention_weights + epsilon), axis=1)\n",
    "        avg_entropy = np.mean(entropy)\n",
    "        \n",
    "        print(f'\\nAverage Attention Entropy: {avg_entropy:.3f}')\n",
    "        print(f'  (Lower entropy = more focused attention)')\n",
    "    \n",
    "    def visualize_multiple_examples(self, data_loader, num_examples=5):\n",
    "        \"\"\"Visualize attention for multiple examples\"\"\"\n",
    "        print(f'\\nVisualizing attention for {num_examples} examples...')\n",
    "        \n",
    "        examples_found = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                if examples_found >= num_examples:\n",
    "                    break\n",
    "                \n",
    "                src = batch['src']\n",
    "                tgt = batch['tgt']\n",
    "                src_texts = batch['src_texts']\n",
    "                tgt_texts = batch['tgt_texts']\n",
    "                \n",
    "                for i in range(src.shape[0]):\n",
    "                    if examples_found >= num_examples:\n",
    "                        break\n",
    "                    \n",
    "                    info, attention_weights = self.visualize_example(\n",
    "                        src[i],\n",
    "                        tgt[i],\n",
    "                        src_texts[i],\n",
    "                        tgt_texts[i],\n",
    "                        examples_found + 1\n",
    "                    )\n",
    "                    \n",
    "                    # Analyze attention patterns\n",
    "                    self.analyze_attention_patterns(\n",
    "                        attention_weights,\n",
    "                        info['src_tokens'],\n",
    "                        info['tgt_tokens']\n",
    "                    )\n",
    "                    \n",
    "                    examples_found += 1\n",
    "        \n",
    "        print(f'\\n{\"=\"*80}')\n",
    "        print(f'Visualized {examples_found} examples')\n",
    "        print(f'Visualizations saved to: {self.viz_dir}')\n",
    "        print(f'{\"=\"*80}')\n",
    "    \n",
    "    def create_summary_visualization(self, data_loader, num_samples=100):\n",
    "        \"\"\"Create summary visualization of attention statistics\"\"\"\n",
    "        print(f'\\nCreating attention summary visualization...')\n",
    "        \n",
    "        all_entropies = []\n",
    "        all_max_attentions = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        samples_processed = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc='Processing'):\n",
    "                if samples_processed >= num_samples:\n",
    "                    break\n",
    "                \n",
    "                src = batch['src'].to(self.device)\n",
    "                src_lengths = batch['src_lengths']\n",
    "                \n",
    "                max_length = self.config['dataset']['max_code_length'] + 2\n",
    "                _, attention_weights = self.model.generate(\n",
    "                    src, src_lengths, max_length, self.tgt_vocab.SOS_token\n",
    "                )\n",
    "                \n",
    "                # Calculate statistics\n",
    "                for i in range(attention_weights.shape[0]):\n",
    "                    if samples_processed >= num_samples:\n",
    "                        break\n",
    "                    \n",
    "                    att = attention_weights[i].cpu().numpy()\n",
    "                    \n",
    "                    # Entropy\n",
    "                    epsilon = 1e-10\n",
    "                    entropy = -np.sum(att * np.log(att + epsilon), axis=1)\n",
    "                    all_entropies.extend(entropy.tolist())\n",
    "                    \n",
    "                    # Max attention\n",
    "                    max_att = np.max(att, axis=1)\n",
    "                    all_max_attentions.extend(max_att.tolist())\n",
    "                    \n",
    "                    samples_processed += 1\n",
    "        \n",
    "        # Create summary plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Entropy distribution\n",
    "        axes[0].hist(all_entropies, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[0].set_xlabel('Attention Entropy', fontsize=12)\n",
    "        axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "        axes[0].set_title('Distribution of Attention Entropy', fontsize=14, fontweight='bold')\n",
    "        axes[0].axvline(np.mean(all_entropies), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(all_entropies):.3f}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Max attention distribution\n",
    "        axes[1].hist(all_max_attentions, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "        axes[1].set_xlabel('Maximum Attention Weight', fontsize=12)\n",
    "        axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "        axes[1].set_title('Distribution of Maximum Attention Weights', fontsize=14, fontweight='bold')\n",
    "        axes[1].axvline(np.mean(all_max_attentions), color='red', linestyle='--',\n",
    "                       label=f'Mean: {np.mean(all_max_attentions):.3f}')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_path = os.path.join(self.viz_dir, 'attention_statistics.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f'Saved attention statistics: {save_path}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Visualize attention weights')\n",
    "    parser.add_argument('--config', type=str, default='config.yaml',\n",
    "                        help='Path to config file')\n",
    "    parser.add_argument('--num_examples', type=int, default=5,\n",
    "                        help='Number of examples to visualize')\n",
    "    parser.add_argument('--summary', action='store_true',\n",
    "                        help='Create summary statistics visualization')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load config\n",
    "    with open(args.config, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Load checkpoint first to check if it exists\n",
    "    checkpoint_path = os.path.join(\n",
    "        config['paths']['checkpoints'],\n",
    "        'attention',\n",
    "        'best_model.pt'\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f'Error: Checkpoint not found: {checkpoint_path}')\n",
    "        print('Please train the attention model first.')\n",
    "        print('\\nTo train the model, run:')\n",
    "        print('  python train.py --config config_quick.yaml --model attention')\n",
    "        return\n",
    "    \n",
    "    # Load checkpoint to get vocabularies and config\n",
    "    print('Loading checkpoint...')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Extract vocabularies from checkpoint\n",
    "    if 'src_vocab' not in checkpoint or 'tgt_vocab' not in checkpoint:\n",
    "        print('Error: Vocabularies not found in checkpoint!')\n",
    "        print('Please retrain the model with the updated train.py')\n",
    "        return\n",
    "    \n",
    "    src_vocab = checkpoint['src_vocab']\n",
    "    tgt_vocab = checkpoint['tgt_vocab']\n",
    "    print(f'Loaded vocabularies from checkpoint: src={src_vocab.n_words}, tgt={tgt_vocab.n_words}')\n",
    "    \n",
    "    # Use config from checkpoint (has correct dimensions)\n",
    "    model_config = checkpoint.get('config', config)\n",
    "    print(f'Using embedding_dim={model_config[\"model\"][\"embedding_dim\"]}, hidden_dim={model_config[\"model\"][\"hidden_dim\"]}')\n",
    "    \n",
    "    # Load test data with checkpoint's vocabularies\n",
    "    print('Loading test data...')\n",
    "    from datasets import load_dataset\n",
    "    from data_loader import CodeSearchNetDataset, collate_fn\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    dataset = load_dataset(\n",
    "        model_config['dataset']['name'],\n",
    "        split='train',\n",
    "        cache_dir=model_config['dataset']['cache_dir']\n",
    "    )\n",
    "    \n",
    "    total_needed = model_config['dataset']['train_size'] + model_config['dataset']['val_size'] + model_config['dataset']['test_size']\n",
    "    dataset = dataset.shuffle(seed=42).select(range(min(total_needed, len(dataset))))\n",
    "    \n",
    "    splits = dataset.train_test_split(test_size=model_config['dataset']['val_size'] + model_config['dataset']['test_size'], seed=42)\n",
    "    temp_splits = splits['test'].train_test_split(test_size=model_config['dataset']['test_size'], seed=42)\n",
    "    test_data = temp_splits['test']\n",
    "    \n",
    "    max_src_len = model_config['dataset'].get('max_docstring_length', 50)\n",
    "    max_tgt_len = model_config['dataset'].get('max_code_length', 100)\n",
    "    test_dataset = CodeSearchNetDataset(test_data, src_vocab, tgt_vocab, max_src_len, max_tgt_len)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=model_config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    print(f'Test dataset size: {len(test_dataset)}')\n",
    "    \n",
    "    src_vocab_size = src_vocab.n_words\n",
    "    tgt_vocab_size = tgt_vocab.n_words\n",
    "    \n",
    "    # Create attention model with checkpoint's config and vocab sizes\n",
    "    print('Creating attention model...')\n",
    "    model = create_attention_seq2seq(src_vocab_size, tgt_vocab_size, model_config, device)\n",
    "    \n",
    "    # Load checkpoint weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]+1}, val_loss={checkpoint[\"val_loss\"]:.4f}')\n",
    "    \n",
    "    # Create visualizer\n",
    "    visualizer = AttentionVisualizer(model, model_config, device, src_vocab, tgt_vocab)\n",
    "    \n",
    "    # Visualize examples\n",
    "    visualizer.visualize_multiple_examples(test_loader, num_examples=args.num_examples)\n",
    "    \n",
    "    # Create summary visualization\n",
    "    if args.summary:\n",
    "        num_samples = min(100, len(test_dataset))\n",
    "        visualizer.create_summary_visualization(test_loader, num_samples=num_samples)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71085e94",
   "metadata": {},
   "source": [
    "### generate_report_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile generate_report_plots.py\n",
    "\"\"\"\n",
    "Generate plots and figures for the report\n",
    "\"\"\"\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "\n",
    "def plot_training_curves(model_names, config):\n",
    "    \"\"\"Plot training and validation loss curves\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(model_names), figsize=(6*len(model_names), 5))\n",
    "    \n",
    "    if len(model_names) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        # Load training history\n",
    "        history_path = os.path.join(config['paths']['logs'], model_name, 'training_history.json')\n",
    "        \n",
    "        if not os.path.exists(history_path):\n",
    "            print(f\"Warning: History not found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        with open(history_path, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        epochs = range(1, len(history['train_losses']) + 1)\n",
    "        \n",
    "        axes[idx].plot(epochs, history['train_losses'], label='Train Loss', linewidth=2)\n",
    "        axes[idx].plot(epochs, history['val_losses'], label='Val Loss', linewidth=2)\n",
    "        axes[idx].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[idx].set_ylabel('Loss', fontsize=12)\n",
    "        axes[idx].set_title(f'{model_name.upper()} - Training Progress', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = os.path.join(config['paths']['results'], 'training_curves.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved training curves: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_model_comparison(model_names, config):\n",
    "    \"\"\"Plot comparison of models across metrics\"\"\"\n",
    "    metrics_data = {\n",
    "        'model': [],\n",
    "        'BLEU': [],\n",
    "        'Token Accuracy': [],\n",
    "        'Exact Match': []\n",
    "    }\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        results_path = os.path.join(config['paths']['results'], model_name, 'evaluation_results.json')\n",
    "        \n",
    "        if not os.path.exists(results_path):\n",
    "            print(f\"Warning: Results not found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        metrics_data['model'].append(model_name.upper())\n",
    "        metrics_data['BLEU'].append(results['overall']['bleu'])\n",
    "        metrics_data['Token Accuracy'].append(results['overall']['token_accuracy'] * 100)\n",
    "        metrics_data['Exact Match'].append(results['overall']['exact_match'] * 100)\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(metrics_data['model']))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    rects1 = ax.bar(x - width, metrics_data['BLEU'], width, label='BLEU Score')\n",
    "    rects2 = ax.bar(x, metrics_data['Token Accuracy'], width, label='Token Accuracy (%)')\n",
    "    rects3 = ax.bar(x + width, metrics_data['Exact Match'], width, label='Exact Match (%)')\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Model Comparison Across Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_data['model'])\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom',\n",
    "                       fontsize=10)\n",
    "    \n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    autolabel(rects3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = os.path.join(config['paths']['results'], 'model_comparison.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved model comparison: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_performance_by_length(model_names, config):\n",
    "    \"\"\"Plot performance by input length\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    metric_names = ['bleu', 'token_accuracy', 'exact_match']\n",
    "    titles = ['BLEU Score by Length', 'Token Accuracy by Length', 'Exact Match by Length']\n",
    "    \n",
    "    for metric_idx, (metric, title) in enumerate(zip(metric_names, titles)):\n",
    "        for model_name in model_names:\n",
    "            results_path = os.path.join(config['paths']['results'], model_name, \n",
    "                                       'evaluation_results.json')\n",
    "            \n",
    "            if not os.path.exists(results_path):\n",
    "                continue\n",
    "            \n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract length buckets\n",
    "            buckets = sorted(results['by_length'].keys())\n",
    "            values = []\n",
    "            \n",
    "            for bucket in buckets:\n",
    "                val = results['by_length'][bucket][metric]\n",
    "                if metric != 'bleu':\n",
    "                    val *= 100\n",
    "                values.append(val)\n",
    "            \n",
    "            axes[metric_idx].plot(buckets, values, marker='o', linewidth=2, \n",
    "                                 label=model_name.upper(), markersize=8)\n",
    "        \n",
    "        axes[metric_idx].set_xlabel('Source Length (tokens)', fontsize=12)\n",
    "        axes[metric_idx].set_ylabel('Score', fontsize=12)\n",
    "        axes[metric_idx].set_title(title, fontsize=14, fontweight='bold')\n",
    "        axes[metric_idx].legend()\n",
    "        axes[metric_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = os.path.join(config['paths']['results'], 'performance_by_length.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved performance by length: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_results_table(model_names, config):\n",
    "    \"\"\"Generate LaTeX table of results\"\"\"\n",
    "    table = \"\\\\begin{table}[h]\\n\"\n",
    "    table += \"\\\\centering\\n\"\n",
    "    table += \"\\\\caption{Model Performance Comparison}\\n\"\n",
    "    table += \"\\\\begin{tabular}{|l|c|c|c|}\\n\"\n",
    "    table += \"\\\\hline\\n\"\n",
    "    table += \"\\\\textbf{Model} & \\\\textbf{BLEU} & \\\\textbf{Token Acc (\\\\%)} & \\\\textbf{Exact Match (\\\\%)} \\\\\\\\\\n\"\n",
    "    table += \"\\\\hline\\n\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        results_path = os.path.join(config['paths']['results'], model_name, \n",
    "                                   'evaluation_results.json')\n",
    "        \n",
    "        if not os.path.exists(results_path):\n",
    "            continue\n",
    "        \n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        bleu = results['overall']['bleu']\n",
    "        token_acc = results['overall']['token_accuracy'] * 100\n",
    "        exact_match = results['overall']['exact_match'] * 100\n",
    "        \n",
    "        table += f\"{model_name.upper()} & {bleu:.2f} & {token_acc:.2f} & {exact_match:.2f} \\\\\\\\\\n\"\n",
    "    \n",
    "    table += \"\\\\hline\\n\"\n",
    "    table += \"\\\\end{tabular}\\n\"\n",
    "    table += \"\\\\end{table}\\n\"\n",
    "    \n",
    "    # Save\n",
    "    save_path = os.path.join(config['paths']['results'], 'results_table.tex')\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(table)\n",
    "    \n",
    "    print(f\"Saved LaTeX table: {save_path}\")\n",
    "    print(\"\\nLaTeX table:\")\n",
    "    print(table)\n",
    "\n",
    "\n",
    "def main():\n",
    "    import yaml\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Generate plots for report')\n",
    "    parser.add_argument('--config', type=str, default='config.yaml')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load config\n",
    "    with open(args.config, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    model_names = ['vanilla', 'lstm', 'attention']\n",
    "    \n",
    "    print(\"Generating plots for report...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Generate all plots\n",
    "    plot_training_curves(model_names, config)\n",
    "    plot_model_comparison(model_names, config)\n",
    "    plot_performance_by_length(model_names, config)\n",
    "    generate_results_table(model_names, config)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"All plots generated successfully!\")\n",
    "    print(f\"Check the '{config['paths']['results']}' directory\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd2a92",
   "metadata": {},
   "source": [
    "### interactive_compare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile interactive_compare.py\n",
    "\"\"\"\n",
    "Interactive Model Comparison\n",
    "Usage:  python interactive_compare.py --config config_colab.yaml\n",
    "        python interactive_compare.py --config config_colab.yaml --input \"sort a list of numbers\"\n",
    "\"\"\"\n",
    "import ast\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "from models import create_vanilla_seq2seq, create_lstm_seq2seq, create_attention_seq2seq\n",
    "\n",
    "RESET  = '\\033[0m'\n",
    "BOLD   = '\\033[1m'\n",
    "GREEN  = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "CYAN   = '\\033[96m'\n",
    "RED    = '\\033[91m'\n",
    "DIM    = '\\033[2m'\n",
    "\n",
    "MODEL_COLOURS = {\n",
    "    'vanilla':   '\\033[94m',\n",
    "    'lstm':      '\\033[95m',\n",
    "    'attention': '\\033[92m',\n",
    "}\n",
    "\n",
    "\n",
    "def load_model(model_name, config, device):\n",
    "    checkpoint_path = os.path.join(\n",
    "        config['paths']['checkpoints'], model_name, 'best_model.pt'\n",
    "    )\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f'{RED}Checkpoint not found for \"{model_name}\": {checkpoint_path}{RESET}')\n",
    "        return None, None, None, None\n",
    "\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "    if 'src_vocab' not in ckpt or 'tgt_vocab' not in ckpt:\n",
    "        print(f'{RED}Vocabularies missing in checkpoint for \"{model_name}\". Retrain first.{RESET}')\n",
    "        return None, None, None, None\n",
    "\n",
    "    src_vocab = ckpt['src_vocab']\n",
    "    tgt_vocab = ckpt['tgt_vocab']\n",
    "    model_cfg = ckpt.get('config', config)\n",
    "\n",
    "    if model_name == 'vanilla':\n",
    "        model = create_vanilla_seq2seq(src_vocab.n_words, tgt_vocab.n_words, model_cfg, device)\n",
    "    elif model_name == 'lstm':\n",
    "        model = create_lstm_seq2seq(src_vocab.n_words, tgt_vocab.n_words, model_cfg, device)\n",
    "    elif model_name == 'attention':\n",
    "        model = create_attention_seq2seq(src_vocab.n_words, tgt_vocab.n_words, model_cfg, device)\n",
    "\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    epoch    = ckpt.get('epoch', '?')\n",
    "    val_loss = ckpt.get('val_loss', float('nan'))\n",
    "    print(f'  {GREEN}checkmark{RESET} {model_name:<12} epoch={epoch+1 if isinstance(epoch,int) else epoch}  val_loss={val_loss:.4f}')\n",
    "    return model, src_vocab, tgt_vocab, model_cfg\n",
    "\n",
    "\n",
    "def encode_input(text, src_vocab, max_len, device):\n",
    "    tokens  = src_vocab.tokenize(text)[:max_len]\n",
    "    indices = [src_vocab.word2idx.get(t, src_vocab.UNK_token) for t in tokens]\n",
    "    indices.append(src_vocab.EOS_token)\n",
    "    src   = torch.tensor([indices], dtype=torch.long).to(device)\n",
    "    src_l = torch.tensor([len(indices)])\n",
    "    return src, src_l\n",
    "\n",
    "\n",
    "def generate(model, model_name, src, src_lengths, tgt_vocab, max_len, device):\n",
    "    with torch.no_grad():\n",
    "        if 'attention' in model_name:\n",
    "            generated, attn = model.generate(src, src_lengths, max_len, tgt_vocab.SOS_token)\n",
    "            attn = attn[0].cpu().numpy()\n",
    "        else:\n",
    "            generated = model.generate(src, src_lengths, max_len, tgt_vocab.SOS_token)\n",
    "            attn = None\n",
    "    indices = generated[0].cpu().tolist()\n",
    "    text    = tgt_vocab.decode(indices)\n",
    "    return text, attn\n",
    "\n",
    "\n",
    "def score_syntax(code):\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def score_bleu(prediction, reference, bleu_scorer):\n",
    "    try:\n",
    "        return bleu_scorer.corpus_score([prediction], [[reference]]).score\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def score_token_overlap(pred_tokens, ref_tokens):\n",
    "    pred_set = set(pred_tokens)\n",
    "    ref_set  = set(ref_tokens)\n",
    "    if not pred_set or not ref_set:\n",
    "        return 0.0\n",
    "    precision = len(pred_set & ref_set) / len(pred_set)\n",
    "    recall    = len(pred_set & ref_set) / len(ref_set)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def print_separator(char='─', width=70):\n",
    "    print(DIM + char * width + RESET)\n",
    "\n",
    "\n",
    "def print_header(text, width=70):\n",
    "    pad = (width - len(text) - 2) // 2\n",
    "    print(BOLD + '─' * pad + f' {text} ' + '─' * (width - pad - len(text) - 2) + RESET)\n",
    "\n",
    "\n",
    "def display_results(input_text, reference, models_output, bleu_scorer, tgt_vocab):\n",
    "    width = 72\n",
    "    print()\n",
    "    print_header('INPUT DOCSTRING', width)\n",
    "    print(f'  {CYAN}{input_text}{RESET}')\n",
    "\n",
    "    if reference:\n",
    "        print_header('REFERENCE CODE', width)\n",
    "        for line in reference.splitlines():\n",
    "            print(f'  {DIM}{line}{RESET}')\n",
    "\n",
    "    print_header('MODEL OUTPUTS', width)\n",
    "    scores = {}\n",
    "    for model_name, (code, attn) in models_output.items():\n",
    "        colour   = MODEL_COLOURS.get(model_name, '')\n",
    "        valid_py = score_syntax(code)\n",
    "        ref_tok  = tgt_vocab.tokenize(reference) if reference else []\n",
    "        pred_tok = tgt_vocab.tokenize(code)\n",
    "        f1       = score_token_overlap(pred_tok, ref_tok) if reference else None\n",
    "        bleu     = score_bleu(code, reference, bleu_scorer) if reference else None\n",
    "\n",
    "        scores[model_name] = {'syntax': valid_py, 'bleu': bleu, 'f1': f1, 'tokens': len(pred_tok)}\n",
    "\n",
    "        print()\n",
    "        print(f'{BOLD}{colour}> {model_name.upper()}{RESET}')\n",
    "        print_separator()\n",
    "        if code.strip():\n",
    "            for line in code.splitlines():\n",
    "                print(f'  {line}')\n",
    "        else:\n",
    "            print(f'  {RED}(empty output){RESET}')\n",
    "\n",
    "        syn_str  = f'{GREEN}valid Python{RESET}' if valid_py else f'{RED}syntax error{RESET}'\n",
    "        tok_str  = f'{len(pred_tok)} tokens'\n",
    "        bleu_str = f'BLEU {bleu:.1f}' if bleu is not None else ''\n",
    "        f1_str   = f'Token-F1 {f1*100:.1f}%' if f1 is not None else ''\n",
    "        details  = '  '.join(filter(None, [syn_str, tok_str, bleu_str, f1_str]))\n",
    "        print(f'  [{details}]')\n",
    "\n",
    "    if reference:\n",
    "        print()\n",
    "        print_header('COMPARISON SUMMARY', width)\n",
    "        print(f'  {\"Model\":<12} {\"Valid Python\":<15} {\"BLEU\":>7}  {\"Token-F1\":>10}  {\"Tokens\":>7}')\n",
    "        print_separator()\n",
    "\n",
    "        ranked = sorted(\n",
    "            scores.items(),\n",
    "            key=lambda x: (x[1]['syntax'], x[1]['bleu'] or 0, x[1]['f1'] or 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        medals = ['#1', '#2', '#3']\n",
    "        for rank, (mn, s) in enumerate(ranked):\n",
    "            colour   = MODEL_COLOURS.get(mn, '')\n",
    "            syn_icon = 'yes' if s['syntax'] else 'no'\n",
    "            bleu_v   = f\"{s['bleu']:>7.2f}\" if s['bleu'] is not None else '    N/A'\n",
    "            f1_v     = f\"{s['f1']*100:>9.1f}%\" if s['f1'] is not None else '      N/A'\n",
    "            medal    = medals[rank] if rank < 3 else '  '\n",
    "            print(f'  {colour}{medal} {mn:<10}{RESET}  {syn_icon:<13}  {bleu_v}  {f1_v}  {s[\"tokens\"]:>7}')\n",
    "\n",
    "        winner = ranked[0][0]\n",
    "        reasons = []\n",
    "        if scores[winner]['syntax']:\n",
    "            reasons.append('valid Python')\n",
    "        if scores[winner]['bleu'] is not None:\n",
    "            reasons.append(f'BLEU {scores[winner][\"bleu\"]:.1f}')\n",
    "        print(f'\\n  Best output: {MODEL_COLOURS[winner]}{winner.upper()}{RESET}  ({\", \".join(reasons)})')\n",
    "\n",
    "    print_separator('=', width)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--config',    type=str, default='config_colab.yaml')\n",
    "    parser.add_argument('--input',     type=str, default=None)\n",
    "    parser.add_argument('--reference', type=str, default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.config, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'\\nLoading models  (device: {device})')\n",
    "    print_separator()\n",
    "\n",
    "    models = {}\n",
    "    vocabs = {}\n",
    "    for name in ['vanilla', 'lstm', 'attention']:\n",
    "        model, src_vocab, tgt_vocab, model_cfg = load_model(name, config, device)\n",
    "        if model is not None:\n",
    "            models[name] = (model, src_vocab, tgt_vocab, model_cfg)\n",
    "            vocabs[name] = (src_vocab, tgt_vocab)\n",
    "\n",
    "    if not models:\n",
    "        print(f'No models loaded — train first: python train.py --config {args.config} --model all')\n",
    "        sys.exit(1)\n",
    "\n",
    "    _, shared_tgt_vocab = list(vocabs.values())[0]\n",
    "    bleu_scorer = BLEU()\n",
    "    max_len = config['dataset']['max_code_length'] + 2\n",
    "    max_src = config['dataset']['max_docstring_length']\n",
    "\n",
    "    print(f'\\nAll models ready.')\n",
    "\n",
    "    if args.input:\n",
    "        model_outputs = {}\n",
    "        for name, (model, src_vocab, tgt_vocab, _) in models.items():\n",
    "            src, src_l = encode_input(args.input, src_vocab, max_src, device)\n",
    "            code, attn = generate(model, name, src, src_l, tgt_vocab, max_len, device)\n",
    "            model_outputs[name] = (code, attn)\n",
    "        display_results(args.input, args.reference or '', model_outputs, bleu_scorer, shared_tgt_vocab)\n",
    "        return\n",
    "\n",
    "    print('\\nType a Python function description and press Enter.')\n",
    "    print('Optionally paste a reference answer on the next prompt.')\n",
    "    print('Type quit or exit to stop.\\n')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            raw = input('Docstring > ').strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print('\\nBye!')\n",
    "            break\n",
    "\n",
    "        if raw.lower() in ('quit', 'exit', 'q'):\n",
    "            print('Bye!')\n",
    "            break\n",
    "        if not raw:\n",
    "            continue\n",
    "\n",
    "        ref = input('Reference code (optional, press Enter to skip) > ').strip()\n",
    "\n",
    "        model_outputs = {}\n",
    "        for name, (model, src_vocab, tgt_vocab, _) in models.items():\n",
    "            src, src_l = encode_input(raw, src_vocab, max_src, device)\n",
    "            code, attn = generate(model, name, src, src_l, tgt_vocab, max_len, device)\n",
    "            model_outputs[name] = (code, attn)\n",
    "\n",
    "        display_results(raw, ref, model_outputs, bleu_scorer, shared_tgt_vocab)\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56167a1a",
   "metadata": {},
   "source": [
    "## Step 4 — Create Config\n",
    "Edit hyperparameters here, then re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f78914",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config_colab.yaml\n",
    "dataset:\n",
    "  name: \"Nan-Do/code-search-net-python\"\n",
    "  train_size: 10000\n",
    "  val_size: 1000\n",
    "  test_size: 1000\n",
    "  max_docstring_length: 50\n",
    "  max_code_length: 80\n",
    "  cache_dir: \"./data_cache\"\n",
    "\n",
    "model:\n",
    "  embedding_dim: 256\n",
    "  hidden_dim: 256\n",
    "  dropout: 0.3\n",
    "  max_vocab_size: 10000\n",
    "\n",
    "training:\n",
    "  batch_size: 64\n",
    "  num_epochs: 5\n",
    "  learning_rate: 0.001\n",
    "  teacher_forcing_ratio: 0.5\n",
    "  gradient_clip: 5.0\n",
    "  save_every: 1\n",
    "\n",
    "paths:\n",
    "  checkpoints: \"./checkpoints\"\n",
    "  results: \"./results\"\n",
    "  visualizations: \"./visualizations\"\n",
    "  logs: \"./logs\"\n",
    "\n",
    "device: \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac57e8c",
   "metadata": {},
   "source": [
    "## Step 5 — Train All 3 Models\n",
    "Takes ~30–60 minutes on T4 GPU. Progress bars show loss per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60761cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --config config_colab.yaml --model all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e7bb2",
   "metadata": {},
   "source": [
    "## Step 6 — Evaluate All Models\n",
    "Computes BLEU, Token Accuracy, Exact Match.\n",
    "Also prints a cross-model comparison table broken down by docstring length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6723942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --config config_colab.yaml --model all --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da1bc8",
   "metadata": {},
   "source": [
    "## Step 7 — Attention Heatmaps & Report Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention heatmaps (5 examples + entropy stats)\n",
    "!python visualize_attention.py --config config_colab.yaml --num_examples 5 --summary\n",
    "\n",
    "# All report plots (training curves, model comparison, performance-by-length)\n",
    "!python generate_report_plots.py --config config_colab.yaml\n",
    "\n",
    "# Display heatmaps inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "\n",
    "viz_dir = \"./visualizations/attention\"\n",
    "png_files = sorted([f for f in os.listdir(viz_dir) if f.endswith(\".png\")])\n",
    "print(f\"Found {len(png_files)} attention visualizations\")\n",
    "\n",
    "fig, axes = plt.subplots(1, min(5, len(png_files)), figsize=(25, 6))\n",
    "if len(png_files) == 1:\n",
    "    axes = [axes]\n",
    "for i, fname in enumerate(png_files[:5]):\n",
    "    img = mpimg.imread(os.path.join(viz_dir, fname))\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(fname.replace(\".png\", \"\"), fontsize=9)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdf864",
   "metadata": {},
   "source": [
    "## Step 8 — View Report Plots Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "\n",
    "plots = [\n",
    "    (\"./results/training_curves.png\",       \"Training Curves\"),\n",
    "    (\"./results/model_comparison.png\",       \"Model Comparison\"),\n",
    "    (\"./results/performance_by_length.png\",  \"Performance by Docstring Length\"),\n",
    "]\n",
    "\n",
    "for fpath, title in plots:\n",
    "    if os.path.exists(fpath):\n",
    "        img = mpimg.imread(fpath)\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(title, fontsize=14)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Not found: {fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7a194",
   "metadata": {},
   "source": [
    "## Step 9 — Interactive Compare\n",
    "\n",
    "Test all three models on any Python docstring you like.  \n",
    "Edit the `docstring` variable in the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1618c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this string and re-run to test any description\n",
    "docstring = \"sort a list of integers in descending order\"\n",
    "\n",
    "!python interactive_compare.py --config config_colab.yaml --input \"{docstring}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e05c349",
   "metadata": {},
   "source": [
    "## Step 9 — Download All Results\n",
    "Downloads three zip files: trained model weights, evaluation results, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "from google.colab import files\n",
    "\n",
    "def zip_folder(folder, zipname):\n",
    "    with zipfile.ZipFile(zipname, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, dirs, fnames in os.walk(folder):\n",
    "            for fname in fnames:\n",
    "                zf.write(os.path.join(root, fname))\n",
    "    print(f\"Zipped: {zipname} ({os.path.getsize(zipname)/1e6:.1f} MB)\")\n",
    "\n",
    "zip_folder(\"./checkpoints\",   \"checkpoints.zip\")\n",
    "zip_folder(\"./results\",       \"results.zip\")\n",
    "zip_folder(\"./visualizations\",\"visualizations.zip\")\n",
    "\n",
    "files.download(\"checkpoints.zip\")\n",
    "files.download(\"results.zip\")\n",
    "files.download(\"visualizations.zip\")\n",
    "print(\"Done! Check your Downloads folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0092c84",
   "metadata": {},
   "source": [
    "## Using Colab Checkpoints Locally\n",
    "\n",
    "After training here on Colab, you can use the trained models **on your local machine** (no GPU needed):\n",
    "\n",
    "### 1. Download the checkpoint zip\n",
    "Run the *Step 10 – Download* cell above.  \n",
    "A file called `checkpoints.zip` will appear in the Colab file browser — right-click → **Download**.\n",
    "\n",
    "### 2. Extract on your local machine\n",
    "Put `checkpoints.zip` inside your `seq2seq/` project folder, then:\n",
    "```\n",
    "# Windows PowerShell\n",
    "Expand-Archive checkpoints.zip -DestinationPath .\n",
    "\n",
    "# Linux / macOS\n",
    "unzip checkpoints.zip\n",
    "```\n",
    "You should now have `checkpoints/vanilla/best_model.pt`, `checkpoints/lstm/best_model.pt`, and `checkpoints/attention/best_model.pt`.\n",
    "\n",
    "### 3. Switch to CPU\n",
    "Open `config_quick.yaml` and change:\n",
    "```yaml\n",
    "device: \"cuda\"   →   device: \"cpu\"\n",
    "```\n",
    "\n",
    "### 4. Run interactive_compare.py locally\n",
    "```\n",
    "python interactive_compare.py --config config_quick.yaml\n",
    "```\n",
    "or for a single shot:\n",
    "```\n",
    "python interactive_compare.py --config config_quick.yaml --input \"reverse a string\"\n",
    "```\n",
    "\n",
    "The checkpoint files carry the full vocabulary inside them, so **no retraining is needed**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
