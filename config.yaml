# Configuration for Seq2Seq Code Generation

# Dataset settings
dataset:
  name: "Nan-Do/code-search-net-python"
  train_size: 10000
  val_size: 1000
  test_size: 1000
  max_docstring_length: 50
  max_code_length: 80
  cache_dir: "./data_cache"

# Model settings
model:
  embedding_dim: 256
  hidden_dim: 256
  dropout: 0.3
  max_vocab_size: 10000

# Training settings
training:
  batch_size: 32
  num_epochs: 20
  learning_rate: 0.001
  teacher_forcing_ratio: 0.5
  gradient_clip: 5.0
  save_every: 1

# Paths
paths:
  checkpoints: "./checkpoints"
  results: "./results"
  visualizations: "./visualizations"
  logs: "./logs"

# Device
device: "cuda" # or "cpu"
