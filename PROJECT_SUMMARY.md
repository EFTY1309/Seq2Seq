# Project Summary: Text-to-Python Code Generation using Seq2Seq Models

## üéØ ASSIGNMENT OBJECTIVE

**Problem Statement:** Implement and compare three Sequence-to-Sequence (Seq2Seq) models to automatically generate Python code from natural language descriptions (docstrings).

**Input:** Natural language description (docstring)

```
"Returns the maximum value in a list"
```

**Output:** Python function code

```python
def max_value(nums):
    return max(nums)
```

---

## üìö WHAT I IMPLEMENTED

### 1. Three Seq2Seq Model Architectures

#### **Model 1: Vanilla RNN Seq2Seq** (Baseline)

- **Location:** `models/vanilla_rnn.py`
- **Architecture:**
  - Encoder: Simple RNN that processes input docstring
  - Decoder: RNN that generates code tokens one by one
  - Fixed context vector passed from encoder to decoder
- **Parameters:** 422,890 trainable parameters
- **Key Limitation:** Fixed-size context vector can lose information for long sequences

#### **Model 2: LSTM Seq2Seq** (Improved Memory)

- **Location:** `models/lstm.py`
- **Architecture:**
  - Encoder: LSTM layers with memory cells (better than RNN at remembering long sequences)
  - Decoder: LSTM layers for generation
  - Fixed context vector with improved gradient flow
- **Parameters:** 621,034 trainable parameters
- **Improvement:** LSTM gates (forget, input, output) help retain important information longer

#### **Model 3: LSTM with Bahdanau Attention** (State-of-the-Art)

- **Location:** `models/attention_lstm.py`
- **Architecture:**
  - Encoder: Bidirectional LSTM (reads input forward and backward)
  - Attention Mechanism: Learns which input words to focus on for each output token
  - Decoder: LSTM with dynamic context (changes for each output token)
- **Parameters:** 1,433,450 trainable parameters
- **Key Innovation:** Instead of fixed context, attention lets decoder "look back" at specific parts of input

---

## üíª IMPLEMENTATION DETAILS

### Complete Codebase Structure

```
seq2seq/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ vanilla_rnn.py       # Model 1 implementation
‚îÇ   ‚îú‚îÄ‚îÄ lstm.py               # Model 2 implementation
‚îÇ   ‚îî‚îÄ‚îÄ attention_lstm.py     # Model 3 implementation with attention
‚îú‚îÄ‚îÄ data_loader.py            # Dataset loading and preprocessing
‚îú‚îÄ‚îÄ train.py                  # Training script for all models
‚îú‚îÄ‚îÄ evaluate.py               # Evaluation with BLEU, accuracy metrics
‚îú‚îÄ‚îÄ visualize_attention.py    # Attention heatmap generation
‚îú‚îÄ‚îÄ config.yaml               # Full training configuration
‚îú‚îÄ‚îÄ config_quick.yaml         # Fast training for testing
‚îî‚îÄ‚îÄ requirements.txt          # All dependencies
```

### Key Components I Built

**1. Data Pipeline (`data_loader.py`)**

- Loads CodeSearchNet dataset (455,243 Python functions)
- Builds vocabulary from training data (519 source words, 1130 target words)
- Tokenizes text and converts to numerical indices
- Handles batching and padding for efficient training

**2. Training Pipeline (`train.py`)**

- Implements teacher forcing (helps model learn faster)
- Gradient clipping to prevent exploding gradients
- Checkpointing (saves best model automatically)
- Loss tracking and validation monitoring

**3. Evaluation System (`evaluate.py`)**

- **BLEU Score:** Standard metric for text generation quality (0-100)
- **Token Accuracy:** Percentage of tokens predicted correctly
- **Exact Match:** Percentage of complete sequences matching reference
- **Length Analysis:** Performance breakdown by input length

**4. Attention Visualization (`visualize_attention.py`)**

- Generates heatmaps showing what input words the model focuses on
- Statistical analysis of attention patterns
- Entropy calculation (measures how focused attention is)

---

## üìä TRAINING PROCESS

### Dataset Used

- **Source:** CodeSearchNet (Hugging Face)
- **Language:** Python functions
- **Total Size:** 455,243 function pairs
- **Training Used:** 100 examples (quick test configuration)
- **Validation:** 20 examples
- **Test:** 20 examples

### Training Configuration

```yaml
Training Data: 100 examples
Epochs: 2 (quick test, normally 10-20)
Embedding Dimension: 128
Hidden Dimension: 128
Batch Size: 8
Learning Rate: 0.001
Device: CPU
```

### Training Results

| Model       | Training Loss | Validation Loss | Training Time |
| ----------- | ------------- | --------------- | ------------- |
| Vanilla RNN | 5.64          | 5.35            | ~2 minutes    |
| LSTM        | 5.97          | 5.42            | ~4 minutes    |
| Attention   | 5.30          | 6.26            | ~10 minutes   |

---

## üìà EVALUATION RESULTS

### Test Set Performance (20 examples)

| Model           | BLEU Score | Token Accuracy | Exact Match |
| --------------- | ---------- | -------------- | ----------- |
| **Vanilla RNN** | 0.12       | 2.75%          | 0.00%       |
| **LSTM**        | 0.15       | 2.79%          | 0.00%       |
| **Attention**   | **0.43**   | 2.35%          | 0.00%       |

### Key Findings

1. **Attention Model Performs Best:**
   - BLEU score of 0.43 (3.6√ó better than vanilla RNN)
   - Better at capturing input-output relationships

2. **Performance by Input Length:**
   - All models perform better on shorter inputs (0-30 tokens)
   - Attention model degrades less on longer sequences

3. **Why Accuracy is Low:**
   - Only trained on 100 examples (normally need thousands)
   - Only 2 epochs (normally 10-20 epochs)
   - Purpose was to demonstrate implementation, not achieve production quality

---

## üé® ATTENTION VISUALIZATIONS

Generated attention heatmaps showing:

- **What:** Which input words model focuses on for each output token
- **Files:** 5 example heatmaps + statistical summary
- **Location:** `visualizations/attention/`

**Example Interpretation:**

- Bright colors (yellow/red) = high attention weight
- Model learns to focus on relevant keywords when generating code
- Can diagnose if model is focusing on correct parts of input

---

## üõ†Ô∏è HOW TO DEMONSTRATE

### 1. Show Training Process

```bash
python train.py --config config_quick.yaml --model all
```

**Shows:** All 3 models training with progress bars and loss values

### 2. Show Evaluation Results

```bash
python evaluate.py --model all --split test
```

**Shows:** Comparison table of BLEU scores and accuracies

### 3. Show Attention Visualizations

```bash
python visualize_attention.py --num_examples 5 --summary
```

**Shows:** Attention heatmaps saved to `visualizations/attention/`

### 4. Show Generated Code Examples

Check files in `results/*/evaluation_results.json` for:

- Source docstring
- Reference (correct) code
- Generated (predicted) code
- Side-by-side comparison

---

## üî¨ TECHNICAL CONCEPTS DEMONSTRATED

### 1. Sequence-to-Sequence Architecture

- Encoder-Decoder paradigm
- Embedding layers for word representations
- Recurrent neural networks for sequential processing

### 2. Advanced RNN Variants

- LSTM cells with gating mechanisms
- Bidirectional processing
- Handling variable-length sequences

### 3. Attention Mechanism

- Bahdanau (additive) attention
- Dynamic context vectors
- Alignment between source and target sequences

### 4. Natural Language Processing

- Tokenization and vocabulary building
- Teacher forcing training strategy
- Beam search for generation (implemented in code)

### 5. Evaluation Metrics

- BLEU score (n-gram overlap)
- Token-level accuracy
- Exact match accuracy

---

## üìù WHAT EACH FILE DOES

| File                       | Purpose                           | Lines of Code    |
| -------------------------- | --------------------------------- | ---------------- |
| `models/vanilla_rnn.py`    | Baseline RNN encoder-decoder      | ~150 lines       |
| `models/lstm.py`           | LSTM-based Seq2Seq                | ~150 lines       |
| `models/attention_lstm.py` | Attention mechanism + LSTM        | ~250 lines       |
| `data_loader.py`           | Dataset loading and preprocessing | ~340 lines       |
| `train.py`                 | Training loop for all models      | ~250 lines       |
| `evaluate.py`              | Evaluation and metrics            | ~360 lines       |
| `visualize_attention.py`   | Attention visualization           | ~390 lines       |
| **Total**                  | **Complete implementation**       | **~1,900 lines** |

