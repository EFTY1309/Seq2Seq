# Configuration for Seq2Seq Code Generation
# FAST TRAINING: ~10-15 minutes total (for quick testing)
# NOTE: For final submission, use config.yaml with 5000+ examples

# Dataset settings
dataset:
  name: "Nan-Do/code-search-net-python"
  train_size: 1000 # Reduced for speed (use 5000+ for final)
  val_size: 200 # Reduced for speed
  test_size: 200 # Reduced for speed
  max_docstring_length: 50 # Assignment requirement
  max_code_length: 80 # Assignment requirement
  cache_dir: "./data_cache"

# Model settings
model:
  embedding_dim: 128 # Smaller for speed (use 256 for final)
  hidden_dim: 128 # Smaller for speed (use 256 for final)
  dropout: 0.3
  max_vocab_size: 5000 # Reduced for speed

# Training settings
training:
  batch_size: 16 # Balanced for speed
  num_epochs: 3 # Minimal for speed (use 15+ for final)
  learning_rate: 0.001
  teacher_forcing_ratio: 0.5
  gradient_clip: 5.0
  save_every: 1 # Save every epoch

# Paths
paths:
  checkpoints: "./checkpoints"
  results: "./results"
  visualizations: "./visualizations"
  logs: "./logs"

# Device
device: "cuda" # or "cpu"
